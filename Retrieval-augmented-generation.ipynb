{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63cf484",
   "metadata": {},
   "source": [
    "# Build an AI Chatbot with LangChain\n",
    "\n",
    "**Author:** Milos Saric [https://saricmilos.com/]  \n",
    "**Date:** January 15, 2026  \n",
    "\n",
    "Here‚Äôs the thing: most ‚Äúhow-to‚Äù guides start with the same line: *‚ÄúGrab your OpenAI API key and add a credit card.‚Äù*  \n",
    "\n",
    "But what if you **don‚Äôt want to pay**? Maybe you‚Äôre a student. Maybe you hate hitting rate limits. Or maybe you just want a chatbot that runs locally, offline, and respects your privacy.  \n",
    "\n",
    "Good news: it‚Äôs possible. In this guide, we‚Äôll show you how to build a chatbot using **LangChain, React, and TypeScript**‚Äîwith **zero cloud dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "## How AI Usually Works ‚Äî APIs and Tokens Explained\n",
    "\n",
    "Think of an **API** as a waiter in a restaurant:\n",
    "\n",
    "1. You ask the app: *‚ÄúWhat‚Äôs the capital of France?‚Äù*  \n",
    "2. The waiter (API) delivers your request to the kitchen (the AI).  \n",
    "3. The kitchen cooks up an answer.  \n",
    "4. The waiter brings it back: *‚ÄúParis!‚Äù*\n",
    "\n",
    "With OpenAI, your computer doesn‚Äôt do the thinking. Everything happens on OpenAI‚Äôs servers. The smarter the AI, the more expensive the ‚Äúmeal.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## Why Tokens Matter ‚Äî Every Word Has a Price\n",
    "\n",
    "Think of each word as a **puzzle piece**. The AI puts the pieces together to understand and respond:\n",
    "\n",
    "- Short phrases: just a couple of pieces.  \n",
    "- Complex sentences: 10‚Äì15 pieces or more.  \n",
    "\n",
    "Every puzzle piece = **token**. APIs like OpenAI charge for tokens, not messages. That‚Äôs why long chats or memory-enabled bots get expensive fast.\n",
    "\n",
    "---\n",
    "\n",
    "### What‚Äôs a Token, Exactly?\n",
    "\n",
    "A token is basically a chunk of text:\n",
    "\n",
    "- `\"Hello\"` ‚Üí 1 token  \n",
    "- `\"Artificial Intelligence is awesome\"` ‚Üí ~5‚Äì6 tokens  \n",
    "- A full conversation ‚Üí hundreds of tokens  \n",
    "- Upload a document ‚Üí thousands of tokens  \n",
    "\n",
    "Example conversation (roughly 50‚Äì80 tokens):\n",
    "\n",
    "**User:** \"Hi, I can't log in to my account\"  \n",
    "**Bot:** \"I can help! What error message are you seeing?\"  \n",
    "**User:** \"'Invalid credentials,' but my password is correct\"  \n",
    "**Bot:** \"Got it. Let's try some troubleshooting steps...\"  \n",
    "\n",
    "Multiply that by hundreds of users, and token costs can explode‚Äîeven at just $0.002 per 1K tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## Hidden Costs of Relying on Paid APIs\n",
    "\n",
    "Even beyond token fees, there are other downsides:\n",
    "\n",
    "### Scaling Costs\n",
    "- Testing & development: ~$50/month  \n",
    "- Small business: $200‚Äì500/month  \n",
    "- Enterprise: $2,000‚Äì10,000+/month  \n",
    "\n",
    "### Infrastructure & Reliability\n",
    "- Your app breaks if the API goes down  \n",
    "- Latency slows the experience  \n",
    "- Constant internet connection required  \n",
    "\n",
    "### Privacy Risks\n",
    "- All data goes through a third-party server  \n",
    "- You can‚Äôt control retention policies  \n",
    "- Compliance issues for sensitive industries  \n",
    "\n",
    "### Service Limits\n",
    "- Rate limits throttle usage  \n",
    "- Outages leave users stranded  \n",
    "- Price changes can break your model  \n",
    "\n",
    "Normally, this is where you‚Äôd start reaching for your wallet. But in this guide, **we‚Äôll avoid all of it**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1778b15",
   "metadata": {},
   "source": [
    "# What is LangChain, Really?\n",
    "\n",
    "LangChain is an **open-source framework** built to help developers create AI applications that are **context-aware, reasoning-capable, and able to use external tools**. The twist? You **don‚Äôt have to rely on cloud APIs** to make it work.  \n",
    "\n",
    "Think of LangChain like **LEGO for AI**. You get different ‚Äúblocks‚Äù that you can snap together:\n",
    "\n",
    "- **Language Model blocks** ‚Äì the brain of your AI  \n",
    "- **Memory blocks** ‚Äì to remember conversations  \n",
    "- **Tool blocks** ‚Äì calculators, web search, file access  \n",
    "- **Logic blocks** ‚Äì decision-making and reasoning  \n",
    "- **Data blocks** ‚Äì document search, databases  \n",
    "\n",
    "Whether your ‚Äúbrain‚Äù is **GPT-4**, a **local model like LLaMA**, or something running entirely on your machine, LangChain helps you assemble all the pieces into a functioning chatbot.  \n",
    "\n",
    "---\n",
    "\n",
    "## Why LangChain is Special\n",
    "\n",
    "Using LangChain with **local or open-source models** gives you benefits you can‚Äôt get with cloud APIs:\n",
    "\n",
    "- ‚úÖ No API key required  \n",
    "- ‚úÖ No token limits ‚Äì chat freely  \n",
    "- ‚úÖ No unexpected bills ‚Äì completely free after setup  \n",
    "- ‚úÖ Full privacy ‚Äì nothing leaves your computer  \n",
    "- ‚úÖ Works offline ‚Äì no internet needed  \n",
    "- ‚úÖ Total control ‚Äì fully customizable  \n",
    "\n",
    "---\n",
    "\n",
    "## Why Use LangChain? (The Big Deal)\n",
    "\n",
    "LangChain isn‚Äôt just about saving money‚Äîit‚Äôs about **flexibility, power, and simplicity**. Whether you‚Äôre a student, beginner, or seasoned developer, here‚Äôs why it‚Äôs a game-changer:\n",
    "\n",
    "\n",
    "### 1. Simplified Development\n",
    "LangChain removes the headache of writing all the low-level code yourself. It handles **connecting models, managing prompts, and storing memory** so you can focus on what your AI should actually do. Think of it as a **pre-built toolbox for AI**, ready to go.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Modular and Flexible Architecture\n",
    "Remember those LEGO blocks? That‚Äôs the core of LangChain‚Äôs design.  \n",
    "\n",
    "- Swap out models or tools with a few lines of code  \n",
    "- Add new data sources easily  \n",
    "- Experiment and iterate faster than ever  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Context Awareness and Memory\n",
    "A good chatbot remembers the conversation. LangChain comes with **built-in memory management**, so your AI keeps track of previous turns, creating **more natural and helpful interactions**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Agentic Capabilities\n",
    "This is where LangChain shines for advanced AI:  \n",
    "\n",
    "- Build AI **agents** that can reason and make decisions  \n",
    "- Use tools automatically: search the web, run calculations, execute code  \n",
    "- Create **multi-step workflows** where the AI can actually solve problems, not just answer questions  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Community and Ecosystem\n",
    "Even though it‚Äôs relatively new, LangChain has a **growing and supportive community**. Tutorials, examples, and resources are abundant. If you get stuck, chances are someone else has faced the same problem.  \n",
    "\n",
    "---\n",
    "\n",
    "LangChain isn‚Äôt just a framework‚Äîit‚Äôs a **platform for building smarter, flexible, and private AI applications** without relying on paid cloud services.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49ad56",
   "metadata": {},
   "source": [
    "# Building a RAG Chatbot with LangChain and OLLAMA APIs\n",
    "\n",
    "Large Language Models (LLMs) are incredibly powerful‚Äîthey can write essays, answer questions, and even get creative‚Äîbut they have one key limitation: **their knowledge is static**. That means they can sometimes give outdated or incorrect answers.  \n",
    "\n",
    "To overcome this, we can use **Retrieval Augmented Generation (RAG)**. RAG systems connect an LLM to **external data sources**, allowing the AI to fetch accurate, up-to-date information before generating a response.  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "The goal of this project is to build a **RAG chatbot** in **LangChain** using **OLLAMA APIs**. The chatbot can:\n",
    "\n",
    "- Accept documents in **TXT, PDF, CSV, or DOCX** formats  \n",
    "- Retrieve relevant content from your uploaded documents  \n",
    "- Provide **accurate, context-aware answers** to your questions by sending the retrieved data to the LLM  \n",
    "\n",
    "This setup ensures your chatbot isn‚Äôt just guessing‚Äîit **leverages your documents to give correct answers**.  \n",
    "\n",
    "---\n",
    "\n",
    "## How the RAG System Works\n",
    "\n",
    "We broke the system down into its main components:\n",
    "\n",
    "1. **Document Loader** ‚Äì Reads and parses your uploaded files  \n",
    "2. **Vector Store / Embeddings** ‚Äì Converts documents into a format that‚Äôs easy for the chatbot to search  \n",
    "3. **Retriever** ‚Äì Finds the most relevant pieces of your documents based on your question  \n",
    "4. **Conversational Retrieval Chain** ‚Äì Combines the retrieved content with your question and passes it to the LLM for an accurate response  \n",
    "\n",
    "---\n",
    "\n",
    "## User Interface\n",
    "\n",
    "To make the system interactive, we built a **React-based UI**. Users can:\n",
    "\n",
    "- Upload documents  \n",
    "- Ask questions about the content  \n",
    "- Receive precise answers generated by the RAG-powered LLM  \n",
    "\n",
    "This creates a **seamless chat experience** where the AI is directly informed by your data.\n",
    "\n",
    "---\n",
    "\n",
    "By combining **LangChain**, **RAG techniques**, and **OLLAMA APIs**, this project demonstrates how to create a chatbot that is **both intelligent and grounded in reliable data**, bridging the gap between static LLM knowledge and real-world information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fe011",
   "metadata": {},
   "source": [
    "![Alt text](./images/rag-chatbot-architecture-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680a288",
   "metadata": {},
   "source": [
    "# Required Libraries Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1f2a6",
   "metadata": {},
   "source": [
    "### CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb2a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea255085",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = Path(\"./data\").resolve()  # main_folder/data\n",
    "\n",
    "TMP_DIR = BASE_DATA_DIR / \"tmp\"  # main_folder/data/tmp\n",
    "DOCS_DIR = BASE_DATA_DIR / \"docs\"  # main_folder/data/docs\n",
    "VECTOR_STORE_DIR = BASE_DATA_DIR / \"vector_stores\"  # main_folder/data/vector_stores\n",
    "\n",
    "# Make sure tmp folder exists\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58970efc",
   "metadata": {},
   "source": [
    "### MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d537a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.API_KEYS import get_environment_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7d09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.langchain import (langchain_document_loader,\n",
    "                            show_random_preview, \n",
    "                            create_text_splitter,\n",
    "                             split_documents,\n",
    "                             tiktoken_tokens,\n",
    "                             select_embeddings_model,\n",
    "                             create_vectorstore,\n",
    "                             print_documents,\n",
    "                             create_advanced_markdown_splitter\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b9ffe",
   "metadata": {},
   "source": [
    "# Required API Keys for Our Application\n",
    "\n",
    "For this project, we will need **two API keys**:\n",
    "\n",
    "- **OpenAI** API key: [Get an API key](https://platform.openai.com/account/api-keys)  \n",
    "- **Google** API key: [Get an API key](https://makersuite.google.com/app/apikey)  \n",
    "\n",
    "> ‚ö†Ô∏è **Security Notice:** We will **not include our secret keys** directly in this notebook.  \n",
    ">\n",
    "> First, make sure to set the following environment variables on your system:  \n",
    "> `OPENAI_API_KEY`, `GOOGLE_API_KEY`.  \n",
    ">\n",
    "> After that, we can safely load them in Python as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5023ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[WARNING]: 'OPENAI_API_KEY' not found in environment variables.\n",
      "[INFO]: 'OPENAI_API_KEY' has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = get_environment_variable(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2968c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milos Saric is a fictional character from the TV show \"The Vampire Diaries.\" He is a vampire hunter who has a vendetta against the vampires in the show.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "try:\n",
    "    client = OpenAI() # defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
    "except:\n",
    "    client = OpenAI(api_key=openai_api_key) # if OPENAI_API_KEY is not created as environment variable\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": 'Who is Milos Saric\"?'},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0ac8a",
   "metadata": {},
   "source": [
    "# Conversational RAG Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f31117",
   "metadata": {},
   "source": [
    "![Alt text](./images/rag_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bce4cd",
   "metadata": {},
   "source": [
    "## RAG Architecture: How Everything Fits Together\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) system is built around **two core building blocks**, each with a clearly defined responsibility.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Block 1: Knowledge Ingestion & Retrieval\n",
    "\n",
    "This block is responsible for **understanding and storing your data** so it can be searched efficiently later. It includes:\n",
    "\n",
    "- **Document Loader** ‚Äì Ingests external data (PDFs, text files, CSVs, DOCX, etc.)  \n",
    "- **Text Splitter** ‚Äì Breaks large documents into smaller, manageable chunks  \n",
    "- **Embedding Model** ‚Äì Converts text chunks into numerical vectors  \n",
    "- **Vector Store (Chroma)** ‚Äì Stores embeddings for fast similarity search  \n",
    "- **Retriever** ‚Äì Finds the most relevant document chunks for a given query  \n",
    "\n",
    "Together, this block transforms raw documents into a searchable knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Block 2: Reasoning, Memory & Generation\n",
    "\n",
    "This block focuses on **thinking, context, and response generation**. It consists of:\n",
    "\n",
    "- **Large Language Model (LLM)** ‚Äì Generates human-like answers  \n",
    "- **Prompt Templates** ‚Äì Structure how questions and retrieved documents are presented to the LLM  \n",
    "- **Memory** ‚Äì Keeps track of conversation history across turns  \n",
    "\n",
    "This block collaborates with the retriever to produce **accurate, context-aware answers**.\n",
    "\n",
    "---\n",
    "\n",
    "## End-to-End RAG Workflow\n",
    "\n",
    "Below is a step-by-step walkthrough of how a user‚Äôs question flows through the RAG system:\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Step 1: Question Reformulation (Steps 1‚Äì4)\n",
    "\n",
    "When a user asks a **follow-up question**, it may rely on prior context.  \n",
    "To avoid ambiguity, the system first converts it into a **standalone question**.\n",
    "\n",
    "**Example:**\n",
    "- User: *‚ÄúWhat does DTC stand for?‚Äù*  \n",
    "- AI: *‚ÄúDTC stands for Diffuse to Choose.‚Äù*  \n",
    "- Follow-up: *‚ÄúCan you explain its use cases and implementation?‚Äù*  \n",
    "\n",
    "The LLM rewrites this as:  \n",
    "> **‚ÄúWhat are the use cases and implementation of Diffuse to Choose (DTC)?‚Äù**\n",
    "\n",
    "This ensures the retriever clearly understands the query.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Step 2: Document Retrieval (Steps 5‚Äì8)\n",
    "\n",
    "- The standalone question is converted into an embedding  \n",
    "- The retriever compares it with stored embeddings in the **Chroma vector database**  \n",
    "- The most relevant document chunks are retrieved based on similarity  \n",
    "\n",
    "This step grounds the AI‚Äôs response in **actual source material**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Step 3: Prompt Augmentation & Answer Generation (Steps 9‚Äì10)\n",
    "\n",
    "- Retrieved documents are injected into the LLM prompt  \n",
    "- Chat history may also be included for continuity  \n",
    "- The augmented prompt is sent to the LLM  \n",
    "\n",
    "The LLM now generates an answer that is **both context-aware and evidence-based**.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Step 4: Memory Update (Step 11)\n",
    "\n",
    "- The user‚Äôs follow-up question and the AI‚Äôs response are saved to memory  \n",
    "- This allows future questions to build naturally on previous interactions  \n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs Next?\n",
    "\n",
    "In the following sections, we‚Äôll take a deeper look at **each individual component**‚Äîfrom document loading and embeddings to retrievers, memory, and conversational chains‚Äîso you can fully understand how to build and customize your own RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00831008",
   "metadata": {},
   "source": [
    "# Conversational RAG Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935bb90",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Retrieval includes: document loaders, text splitting into chunks, vector stores and embeddings, and finally retrievers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce461c",
   "metadata": {},
   "source": [
    "# How to Add More Personal Information to Train Your RAG System\n",
    "\n",
    "## First: A Key Mindset Shift\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) does **not** train or fine-tune the language model itself.  \n",
    "Instead, it improves responses by **retrieving relevant documents** and injecting them into the model‚Äôs prompt.\n",
    "\n",
    "> In simple terms:  \n",
    "> **You don‚Äôt train the model ‚Äî you train the knowledge base.**\n",
    "\n",
    "The quality of your RAG chatbot depends directly on the **quality, structure, and relevance** of the documents you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## What Kind of Information Should You Add About Yourself?\n",
    "\n",
    "To get the best results, organize your personal data into **clear categories** rather than one large, unstructured file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27b257",
   "metadata": {},
   "source": [
    "## Document Loaders in LangChain\n",
    "\n",
    "Document loaders are a core component of LangChain. Their job is simple but critical: **they ingest data from different sources and convert it into a standardized document format** that the rest of the pipeline can understand.\n",
    "\n",
    "LangChain provides **80+ built-in document loaders**, making it easy to work with data from almost anywhere, including:\n",
    "\n",
    "- üåê Web pages and APIs  \n",
    "- ‚òÅÔ∏è Cloud storage services (e.g., AWS S3)  \n",
    "- üìÅ Local files such as TXT, PDF, CSV, and JSON  \n",
    "- üßæ Git repositories  \n",
    "- üìß Emails and messaging platforms  \n",
    "- üóÑÔ∏è Databases and other structured sources  \n",
    "\n",
    "You can explore the full list here:  \n",
    "üëâ [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Document Loaders Matter\n",
    "\n",
    "Raw data comes in many formats. Document loaders handle:\n",
    "- File reading and parsing  \n",
    "- Format-specific preprocessing  \n",
    "- Converting data into LangChain‚Äôs `Document` objects  \n",
    "\n",
    "This ensures downstream components‚Äîlike text splitters, embeddings, and retrievers‚Äîcan work seamlessly regardless of the data source.\n",
    "\n",
    "---\n",
    "\n",
    "## Document Loaders in Our Application\n",
    "\n",
    "For our application, we use the **`DirectoryLoader`** to load files from a temporary directory (`TMP_DIR`).\n",
    "\n",
    "This approach allows us to:\n",
    "- Upload multiple files at once  \n",
    "- Support different document formats  \n",
    "- Process all files with a single loader  \n",
    "\n",
    "Supported formats include:\n",
    "- `.txt`\n",
    "- `.pdf`\n",
    "- `.csv`\n",
    "- `.docx`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd25e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Load Summary ---\n",
      "Total Documents: 23\n",
      "  ‚Ä¢ MD: 23 files\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Load documents\n",
    "documents = langchain_document_loader(DOCS_DIR)\n",
    "\n",
    "# Professional summary reporting\n",
    "if documents:\n",
    "    counts = Counter(doc.metadata.get('source', '').split('.')[-1] for doc in documents)\n",
    "    \n",
    "    print(f\"--- Load Summary ---\")\n",
    "    print(f\"Total Documents: {len(documents)}\")\n",
    "    for ext, count in counts.items():\n",
    "        print(f\"  ‚Ä¢ {ext.upper()}: {count} files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents found. Check your directory path and file extensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24194666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf05d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Document[14]** \n",
       "\n",
       " **Page content** (first 1000 character):\n",
       "\n",
       "# Pricing & Packages ‚Äì Cassiopeia Intelligence\n",
       "\n",
       "This document provides a **detailed, enterprise-grade overview of pricing and packages** for AI and RAG-powered solutions, ensuring transparency and clarity for clients.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Chatbots\n",
       "\n",
       "* **Basic Package:**\n",
       "\n",
       "  * Single-channel chatbot (web or mobile)\n",
       "  * FAQ automation and standard multi-turn conversations\n",
       "  * RAG integration with a single knowledge source\n",
       "  * Price: $25,000 ‚Äì $50,000\n",
       "  * Delivery: 4‚Äì6 weeks\n",
       "\n",
       "* **Standard Package:**\n",
       "\n",
       "  * Multi-channel deployment (web, mobile, social media)\n",
       "  * Multi-turn RAG-powered conversations\n",
       "  * Knowledge base integration across multiple documents\n",
       "  * Analytics dashboard and reporting\n",
       "  * Price: $50,000 ‚Äì $100,000\n",
       "  * Delivery: 8‚Äì10 weeks\n",
       "\n",
       "* **Enterprise Package:**\n",
       "\n",
       "  * Advanced multi-turn RAG chatbot with personalized responses\n",
       "  * Multi-source knowledge retrieval and vector search\n",
       "  * Integration with CRM, ticketing, and ERP systems\n",
       "  * Custom analytics and monitoring dashboards\n",
       "  * Price: $1 ...\n",
       "\n",
       "**Metadata:**\n",
       "\n",
       "{'source': 'C:\\\\Users\\\\Milos\\\\Desktop\\\\GitHub_Kaggle_Projects\\\\simple_RAG_assistant_with_Langchain\\\\data\\\\docs\\\\pricing.md'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import Markdown\n",
    "random_document_id = random.choice(range(len(documents)))\n",
    "\n",
    "Markdown(f\"**Document[{random_document_id}]** \\n\\n **Page content** (first 1000 character):\\n\\n\" +\\\n",
    "         documents[random_document_id].page_content[0:1000] + \" ...\"  +\\\n",
    "         \"\\n\\n**Metadata:**\\n\\n\" + str(documents[random_document_id].metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70b3e9",
   "metadata": {},
   "source": [
    "# TEXT SPLITTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e27ef",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"text_splitters\">Text Splitters</a>\n",
    "\n",
    "Text splitters are tools that divide large documents into smaller sections, or **chunks**, that fit within a model's context window. Since language models can only process a limited number of tokens at a time, splitting text effectively is crucial for maintaining context and ensuring high-quality responses.\n",
    "\n",
    "In **LangChain**, text can be split in several ways:\n",
    "\n",
    "- **By tokens** ‚Äì divides text based on the number of tokens used by the model.  \n",
    "- **By characters** ‚Äì splits text according to character counts.  \n",
    "- **By code structure** ‚Äì specialized splitters exist for programming languages like Java, JavaScript, and PHP, allowing chunks to respect logical code blocks.  \n",
    "\n",
    "### Recommended Splitter for General Text\n",
    "\n",
    "For most text documents, it is recommended to use the **[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)**.  \n",
    "\n",
    "- **How it works:**  \n",
    "  The splitter uses a **list of characters or strings** (for example, `\"\\n\\n\"`, `\"\\n\"`, `\" \"`) in a specific order to recursively divide the text. It continues splitting until each chunk is small enough to fit within the model‚Äôs context window.  \n",
    "\n",
    "- **Why it‚Äôs effective:**  \n",
    "  This method preserves **semantic relationships** between paragraphs, sentences, and words by keeping related content together as much as possible. Instead of splitting arbitrarily, it prioritizes meaningful boundaries.\n",
    "\n",
    "### Chunk Overlap\n",
    "\n",
    "To ensure consistency and improve context retention, a **small overlap** between consecutive chunks is recommended. This means that some content at the end of one chunk is repeated at the start of the next.  \n",
    "\n",
    "- **Benefits:**  \n",
    "  - Helps the model maintain context across chunks.  \n",
    "  - Reduces the chance of losing important information between splits.  \n",
    "\n",
    "By using a proper text splitter and overlap, you can maximize the usefulness of documents in **retrieval-augmented generation (RAG)** workflows and other applications requiring structured text input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef633710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Splitters\n",
    "md_splitter, rec_splitter = create_advanced_markdown_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ffd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Two-Stage Splitting Process\n",
    "final_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d876e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created 205 semantic chunks ready for the Vector Store.\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    # STAGE A: Structural Split (Header-aware)\n",
    "    # Note: .split_text() returns a list of Document objects with header metadata\n",
    "    header_docs = md_splitter.split_text(doc.page_content)\n",
    "    \n",
    "    # Optional: Attach original file metadata (like 'source') to each header split\n",
    "    for h_doc in header_docs:\n",
    "        h_doc.metadata.update(doc.metadata)\n",
    "        \n",
    "    # STAGE B: Size-based Split (Recursively to fit 1600 characters)\n",
    "    # .split_documents() accepts the list of Docs from Stage A\n",
    "    sub_chunks = rec_splitter.split_documents(header_docs)\n",
    "    \n",
    "    final_chunks.extend(sub_chunks)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(final_chunks)} semantic chunks ready for the Vector Store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242151eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1070, 932, 1291, 1174, 1432, 487, 671, 759, 771, 635, 709, 714, 635, 610, 811, 1043, 1068, 534, 1054, 946, 866, 727, 727]\n"
     ]
    }
   ],
   "source": [
    "token_counts = tiktoken_tokens(documents)\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd66925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens - Average : 95\n",
      "Number of tokens - 25% percentile : 52\n",
      "Number of tokens - 50% percentile : 86\n",
      "Number of tokens - 75% percentile : 127\n",
      "\n",
      "Max_tokens for gpt-3.5-turbo: 4096\n"
     ]
    }
   ],
   "source": [
    "chunks_length = tiktoken_tokens(final_chunks,model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"Number of tokens - Average : {int(np.mean(chunks_length))}\")\n",
    "print(f\"Number of tokens - 25% percentile : {int(np.quantile(chunks_length,0.25))}\")\n",
    "print(f\"Number of tokens - 50% percentile : {int(np.quantile(chunks_length,0.5))}\")\n",
    "print(f\"Number of tokens - 75% percentile : {int(np.quantile(chunks_length,0.75))}\")\n",
    "print(\"\\nMax_tokens for gpt-3.5-turbo: 4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28336298",
   "metadata": {},
   "source": [
    "# Vectorsores and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da18bd4",
   "metadata": {},
   "source": [
    "### Text Embeddings\n",
    "\n",
    "**Text embeddings** are numerical representations of text in a high-dimensional vector space. In simpler terms, they convert words, sentences, or entire documents into a list of numbers (vectors) that capture their **semantic meaning**.  \n",
    "\n",
    "For example, OpenAI‚Äôs `text-embedding-ada-002` model produces embeddings of size **1536**, meaning each text input is represented as a vector with 1,536 numbers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why embeddings matter\n",
    "\n",
    "Embeddings allow us to **compare the meaning of different texts**. Once text is converted into vectors, we can measure similarity between them using mathematical techniques.  \n",
    "\n",
    "- **Cosine similarity** is the most commonly used metric.  \n",
    "  - Values range from `-1` (completely opposite) to `1` (exactly similar).  \n",
    "  - Higher cosine similarity means the texts are more semantically alike.  \n",
    "\n",
    "This is especially useful for:\n",
    "- **Semantic search** ‚Äì finding documents most relevant to a query.  \n",
    "- **Recommendation systems** ‚Äì suggesting similar content.  \n",
    "- **Clustering and classification** ‚Äì grouping similar texts together.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Embedding providers\n",
    "\n",
    "Several platforms provide pre-trained embedding models. LangChain supports easy integration with these services:\n",
    "\n",
    "| Provider       | Model | Vector dimension | Notes / Cost |\n",
    "|----------------|-------|----------------|--------------|\n",
    "| OpenAI         | [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/embedding-models) | 1536 | **$0.00010 per 1K tokens** |\n",
    "| Google         | [models/embedding-001](https://ai.google.dev/models/gemini?hl=en) | 768 | **Rate limit:** 1500 requests per minute |\n",
    "| Hugging Face   | [thenlper/gte-large](https://huggingface.co/thenlper/gte-large) | 1024 | **Free** |\n",
    "\n",
    "> üí° **Tip:** Larger vector dimensions often capture more nuanced meaning but may be more computationally expensive for storage and similarity calculations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a92c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_openai = select_embeddings_model(LLM_service=\"OpenAI\", openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349f1894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between sentence 0 and 1: 0.899\n",
      "Similarity between sentence 0 and 2: 0.711\n",
      "Similarity between sentence 1 and 2: 0.712\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I want to become the world greatest data scientist.\",\n",
    "    \"I love data science.\",\n",
    "    \"How many hours should you walk per day?\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Generate embeddings\n",
    "# -------------------------------\n",
    "# embeddings_google is assumed to be initialized already\n",
    "embedding_vectors = [embeddings_openai.embed_query(sentence) for sentence in sentences]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Calculate pairwise similarity\n",
    "# -------------------------------\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Iterate over all sentence pairs\n",
    "for i, j in combinations(range(len(sentences)), 2):\n",
    "    sim_score = round(cosine_similarity(embedding_vectors[i], embedding_vectors[j]), 3)\n",
    "    print(f\"Similarity between sentence {i} and {j}: {sim_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92914301",
   "metadata": {},
   "source": [
    "### Vectorstores\n",
    "\n",
    "A **vectorstore** is a specialized type of database designed to store **embedding vectors** ‚Äî the numerical representations of text, images, or other data.  \n",
    "\n",
    "Unlike traditional databases that search by exact matches or keywords, vectorstores allow you to **search for the items that are most semantically similar** to a query. This is done by comparing the embeddings of the query with the stored embeddings, typically using metrics like **cosine similarity** or **Euclidean distance**.\n",
    "\n",
    "There are several open-source and commercial vectorstore options available. For this guide, we will use **[Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma)**, a lightweight and efficient vector database that integrates seamlessly with LangChain.\n",
    "\n",
    "> üí° **Tip:** Vectorstores are a key component of modern AI workflows, such as **semantic search**, **question answering**, and **retrieval-augmented generation (RAG)**, because they allow models to find relevant information quickly and accurately based on meaning, not just keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b843a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted at: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\vector_stores\\milossaric_vectorstore\n"
     ]
    }
   ],
   "source": [
    "vectorstore_name = \"milossaric_vectorstore\"\n",
    "vector_store = create_vectorstore(\n",
    "    embeddings=embeddings_openai,\n",
    "    documents=final_chunks,\n",
    "    vectorstore_name=vectorstore_name,\n",
    "    vectorstore_dir=VECTOR_STORE_DIR\n",
    ")\n",
    "\n",
    "print(f\"Vector store created and persisted at: {VECTOR_STORE_DIR / vectorstore_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "938928c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_OpenAI: 236 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milos\\AppData\\Local\\Temp\\ipykernel_8336\\2820668442.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_store_OpenAI = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Define the full path to the persisted vector store\n",
    "vector_store_path = VECTOR_STORE_DIR / \"milossaric_vectorstore\"\n",
    "\n",
    "# Load the persisted vector store\n",
    "vector_store_OpenAI = Chroma(\n",
    "    persist_directory=vector_store_path.as_posix(),\n",
    "    embedding_function=embeddings_openai\n",
    ")\n",
    "\n",
    "# Print the number of vectors/chunks in the store\n",
    "print(\"vector_store_OpenAI:\", vector_store_OpenAI._collection.count(), \"chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7ba7c",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "`Similarity search` is a technique used to find documents that are most relevant or similar to a given query. Unlike simple keyword matching, it relies on **embedding vectors**, which are numerical representations of the semantic meaning of text.  \n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Query Embedding:**  \n",
    "   The input question or query is converted into an embedding vector using the same embedding model as the vector store. This vector captures the meaning of the query in a numerical form.\n",
    "\n",
    "2. **Comparison with Vector Store:**  \n",
    "   The query embedding is compared against all document embeddings stored in the vector store. Similarity is usually measured using metrics like **cosine similarity**, which evaluates how close vectors are in the embedding space.\n",
    "\n",
    "3. **Selecting Top Matches:**  \n",
    "   The system selects the **k most similar documents** to the query (by default, k = 4). These documents are considered most likely to contain information relevant to the query.\n",
    "\n",
    "4. **Providing Context to LLMs:**  \n",
    "   The retrieved documents are sent along with the query to a **large language model (LLM)** ‚Äî such as ChatGPT, Google Gemini, or others. Providing these relevant documents helps the LLM generate **more accurate and informed responses**.\n",
    "\n",
    "5. **Optimizing Retrieval:**  \n",
    "   To improve efficiency and relevance, we can later apply **contextual compression**, which condenses the retrieved documents to only the most essential information while preserving context for the LLM.\n",
    "\n",
    "**Summary:**  \n",
    "Similarity search acts as a **bridge between your knowledge base and the LLM**, ensuring that the model has access to the most relevant information when answering queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9838a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "Score: 0.293\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "Score: 0.354\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "Score: 0.355\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "Score: 0.370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Who can make me a recommendation system?\"\n",
    "\n",
    "# Retrieve the top 4 most similar documents with scores\n",
    "# Using cosine similarity (lower distance = more similar)\n",
    "docs_with_scores = vector_store_OpenAI.similarity_search_with_score(query, k=4)\n",
    "\n",
    "# Print the results with scores\n",
    "print_documents(docs_with_scores, search_with_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2cc84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_and_log_relevance(query, docs_with_scores, embeddings_model):\n",
    "    \"\"\"\n",
    "    Computes semantic similarity using a vectorized dot product \n",
    "    and logs a formatted relevance report.\n",
    "    \"\"\"\n",
    "    # 1. Generate Embeddings (Vectorized)\n",
    "    query_vector = embeddings_model.embed_query(query)\n",
    "    document_texts = [doc[0].page_content for doc in docs_with_scores]\n",
    "    doc_matrix = np.array(embeddings_model.embed_documents(document_texts))\n",
    "\n",
    "    # 2. Vectorized Similarity Calculation (Dot Product)\n",
    "    # Using np.inner for high-performance vector-matrix multiplication\n",
    "    relevance_scores = np.inner(query_vector, doc_matrix)\n",
    "\n",
    "    # 3. Professional Report Formatting\n",
    "    print(f\"{'='*20} RETRIEVAL RELEVANCE REPORT {'='*20}\")\n",
    "    print(f\"QUERY: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    for idx, score in enumerate(relevance_scores):\n",
    "        source = docs_with_scores[idx][0].metadata.get('source', 'Unknown')\n",
    "        # Display as normalized relevance (assuming unit vectors)\n",
    "        print(f\"RANK {idx+1} | SCORE: {score:.4f} | SOURCE: {source}\")\n",
    "        \n",
    "    print(f\"{'='*68}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9756d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== RETRIEVAL RELEVANCE REPORT ====================\n",
      "QUERY: \"Who can make me a recommendation system?\"\n",
      "\n",
      "RANK 1 | SCORE: 0.8533 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "RANK 2 | SCORE: 0.8229 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "RANK 3 | SCORE: 0.8224 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\recommendation-systems.md\n",
      "RANK 4 | SCORE: 0.8152 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\recommendation-system-implementation.md\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute the reporting function\n",
    "compute_and_log_relevance(query, docs_with_scores, embeddings_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb72d28",
   "metadata": {},
   "source": [
    "# Maximum Marginal Relevance (MMR) Search\n",
    "\n",
    "Maximum Marginal Relevance (MMR) is a technique used in search and information retrieval to **select documents that are both relevant to a query and diverse among themselves**. This helps avoid redundancy in search results.\n",
    "\n",
    "- **Relevance:** How closely a document matches the query.\n",
    "- **Diversity:** How different each selected document is from the others.\n",
    "\n",
    "MMR balances these two aspects. It ensures that the selected documents are not only similar to your query but also cover different perspectives or topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3fabdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Model Selection  \n",
      "### 2.1 Classical Models  \n",
      "* **Collaborative Filtering:** User-based, item-based, matrix factorization (SVD, ALS)\n",
      "* **Content-Based Models:** TF-IDF, BM25, feature-based similarity\n",
      "* **Hybrid Approaches:** Weighted, cascade, or switching models\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "## 5. Deployment Options  \n",
      "### 5.1 Cloud Deployment  \n",
      "* AWS, GCP, Azure for scalable, managed infrastructure\n",
      "* Serverless functions for event-driven RAG pipelines\n",
      "* Kubernetes (EKS, GKE, AKS) for containerized microservices\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "### 1.3 Tailwind CSS  \n",
      "* Utility-first CSS framework for rapid UI development and consistent design language\n",
      "* Enables responsive and adaptive design without heavy custom CSS\n",
      "* Works seamlessly with component libraries for maintainable and scalable styling\n",
      "* Reduces CSS bloat, improving rendering performance and load times  \n",
      "**Enterprise Metrics:**  \n",
      "* SSR with Next.js reduces initial load time by 40‚Äì60%\n",
      "* Tailwind CSS decreases CSS bundle size by 30‚Äì50%, improving page rendering efficiency  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Text/Photo/Video (Advanced): LaTeX, Adobe AÔøΩer EÔ¨Äects, DaVinci Resolve, MS OÔ¨Éce \n",
      "Power Systems (Familiar): MATLAB, Simulink, PSS/E, ETAP , GridCal/Pandapower \n",
      " \n",
      "Document 6 ‚Äî SoÔøΩ Skills \n",
      "‚Ä¢ PresentaÔøΩon, Storytelling, Business CommunicaÔøΩon: clear, engaging delivery; simplify \n",
      "complex topics \n",
      "‚Ä¢ Mentoring & Teaching: guiding students & teams eÔ¨ÄecÔøΩvely \n",
      "‚Ä¢ Languages: English, Serbian, Spanish \n",
      " \n",
      "Document 7 ‚Äî Key Stats \n",
      "‚Ä¢ Experience: 7+ years in data science, AI, and power systems \n",
      "‚Ä¢ Projects Completed: 15+ \n",
      "‚Ä¢ Languages & Frameworks: 6+ \n",
      "‚Ä¢ Published PlaÔøΩorm: 1 \n",
      "‚Ä¢ EducaÔøΩon: MSc & BSc in Engineering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'what is Diffuse to Choose?'\n",
    "docs_MMR = vector_store_OpenAI.max_marginal_relevance_search(query,k=4)\n",
    "\n",
    "print_documents(docs_MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4d1cd",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"retrievers\">Retrievers</a>\n",
    "\n",
    "A **retriever** is a component in a search or question-answering system that is responsible for **finding and returning documents relevant to a user‚Äôs query**.  \n",
    "\n",
    "Think of a retriever as the first step in a system that answers questions: it **finds the right documents**, which can then be processed further, e.g., summarized or used by a language model.\n",
    "\n",
    "For more details, you can check the [LangChain retrievers documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/).\n",
    "\n",
    "---\n",
    "\n",
    "### How Retrievers Work\n",
    "\n",
    "1. **Receive a query**: The retriever takes a user query as input.\n",
    "2. **Search documents**: It searches a collection of documents (could be a database, a vector store, or a traditional search engine).\n",
    "3. **Return relevant documents**: It outputs documents that are most likely to answer the query.\n",
    "\n",
    "> Retrievers **do not generate new text**; they only fetch existing information.  \n",
    "\n",
    "---\n",
    "\n",
    "### Types of Retrievers\n",
    "\n",
    "1. **Vectorstore-backed Retriever** (Semantic Search)  \n",
    "   - Uses **embeddings** to represent documents and queries as high-dimensional vectors.\n",
    "   - Measures similarity between query vectors and document vectors.\n",
    "   - Returns documents that are **semantically similar**, even if exact words do not match.\n",
    "\n",
    "2. **Keyword-based Retriever**  \n",
    "   - Uses traditional keyword matching (like search engines or databases).\n",
    "   - Simple but may miss documents that **use different wording** for the same concept.\n",
    "\n",
    "3. **Hybrid Retriever**  \n",
    "   - Combines vector search and keyword search for **more accurate retrieval**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c059dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal, Dict, Any\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "def get_vectorstore_retriever(\n",
    "    vectorstore: VectorStore,\n",
    "    search_type: Literal[\"similarity\", \"mmr\", \"similarity_score_threshold\"] = \"similarity\",\n",
    "    k: int = 4,\n",
    "    score_threshold: Optional[float] = None,\n",
    "    fetch_k: int = 20\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"Standardizes the instantiation of vectorstore-backed retrievers.\n",
    "\n",
    "    Args:\n",
    "        vectorstore: The initialized LangChain VectorStore instance.\n",
    "        search_type: Algorithm for retrieval. \n",
    "            - 'similarity': Standard cosine/L2 distance.\n",
    "            - 'mmr': Max Marginal Relevance (diversifies results).\n",
    "            - 'similarity_score_threshold': Filters results by absolute score.\n",
    "        k: The number of final documents to return to the LLM.\n",
    "        score_threshold: Minimum relevance score required (range 0.0 to 1.0).\n",
    "            Only utilized when search_type is 'similarity_score_threshold'.\n",
    "        fetch_k: Amount of documents to pass to the MMR algorithm for reranking.\n",
    "\n",
    "    Returns:\n",
    "        BaseRetriever: A configured retriever object ready for RAG chains.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define search parameters with sensible defaults for enterprise RAG\n",
    "    search_kwargs: Dict[str, Any] = {\"k\": k}\n",
    "    \n",
    "    if search_type == \"mmr\":\n",
    "        search_kwargs[\"fetch_k\"] = fetch_k\n",
    "        \n",
    "    if score_threshold is not None:\n",
    "        search_kwargs[\"score_threshold\"] = score_threshold\n",
    "\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=search_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc220b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search\n",
    "base_retriever_OpenAI = get_vectorstore_retriever(vector_store_OpenAI,\"similarity\",k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d54694d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "# Recommendation Systems Overview ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **detailed, professional overview of recommendation systems**, emphasizing enterprise-grade architectures, RAG integration, and measurable business impact. It is designed for internal knowledge, client presentations, and technical strategy alignment.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "### 1.3 Hybrid Recommendation Systems  \n",
      "**Description:**\n",
      "Hybrid systems combine collaborative and content-based approaches to maximize accuracy and coverage.  \n",
      "**Common Architectures:**  \n",
      "* Weighted hybrid: Combine scores from multiple models\n",
      "* Switching hybrid: Choose model based on context (e.g., cold-start vs active user)\n",
      "* Cascade hybrid: Rank candidates from one model, re-rank using another  \n",
      "**Enterprise Metrics:**  \n",
      "* Netflix hybrid approach improves RMSE by 10‚Äì15% over pure collaborative filtering\n",
      "* Amazon reports increased conversion rates by combining behavioral signals with product content embeddings  \n",
      "**RAG Integration:**  \n",
      "* Uses vector retrieval to enhance embeddings from multiple sources (user behavior + content metadata + support documents)\n",
      "* Supports multi-domain recommendations for enterprise applications  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "## 3. Example Applications  \n",
      "* **E-commerce:** Personalized product recommendations (Amazon, Alibaba)\n",
      "* **Media & Streaming:** Content suggestions (Netflix, Spotify, YouTube)\n",
      "* **SaaS Platforms:** Feature or content recommendations within enterprise tools\n",
      "* **Knowledge Management:** Suggesting relevant documents, SOPs, and policies for employees\n",
      "* **Sales Enablement:** Lead prioritization and cross-selling recommendations\n",
      "* **Conversational Agents:** RAG-powered chatbots providing context-aware suggestions in multi-turn dialogues  \n",
      "**Enterprise Metrics:**  \n",
      "* Average increase in click-through rate: 15‚Äì25%\n",
      "* Average revenue lift: 10‚Äì20%\n",
      "* Reduction in search effort: 30‚Äì50%  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "# Recommendation Systems Overview ‚Äì Cassiopeia Intelligence\n",
      "\n",
      "This document provides a **detailed, professional overview of recommendation systems**, emphasizing enterprise-grade architectures, RAG integration, and measurable business impact. It is designed for internal knowledge, client presentations, and technical strategy alignment.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Types of Recommendation Systems\n",
      "\n",
      "### 1.1 Collaborative Filtering\n",
      "\n",
      "**Description:**\n",
      "Collaborative filtering leverages user-item interactions to make predictions, identifying patterns in behavior across users.\n",
      "\n",
      "**Key Approaches:**\n",
      "\n",
      "* **User-based:** Recommends items liked by similar users\n",
      "* **Item-based:** Recommends items similar to those a user has interacted with\n",
      "* **Matrix factorization (SVD, ALS):** Reduces dimensionality for large datasets\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Netflix: Collaborative filtering achieves 60‚Äì70% of recommendation accuracy\n",
      "* Amazon: Item-based filtering scales to millions of products and users with >80% relevance in click-through\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Enhances collaborative models with retrieved context from product metadata or knowledge bases\n",
      "* Reduces cold-start problem by providing auxiliary information for new items/users\n",
      "\n",
      "---\n",
      "\n",
      "### 1.2 Content-Based Filtering\n",
      "\n",
      "**Description:**\n",
      "Content-based systems recommend items similar to those a user has interacted with, based on item features and metadata.\n",
      "\n",
      "**Techniques:**\n",
      "\n",
      "* TF-IDF, BM25 for textual content similarity\n",
      "* Embedding-based similarity using BERT or sentence transformers\n",
      "* Feature weighting and attribute-based scoring\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Spotify uses content-based filtering to surface new songs aligned with listener preferences\n",
      "* LinkedIn applies content-based recommendations to match users with relevant content or job postings\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Retrieves additional context from knowledge bases (product details, reviews, technical documentation) to enrich recommendations\n",
      "* Enables explainable recommendations by grounding suggestions in documented features\n",
      "\n",
      "---\n",
      "\n",
      "### 1.3 Hybrid Recommendation Systems\n",
      "\n",
      "**Description:**\n",
      "Hybrid systems combine collaborative and content-based approaches to maximize accuracy and coverage.\n",
      "\n",
      "**Common Architectures:**\n",
      "\n",
      "* Weighted hybrid: Combine scores from multiple models\n",
      "* Switching hybrid: Choose model based on context (e.g., cold-start vs active user)\n",
      "* Cascade hybrid: Rank candidates from one model, re-rank using another\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Netflix hybrid approach improves RMSE by 10‚Äì15% over pure collaborative filtering\n",
      "* Amazon reports increased conversion rates by combining behavioral signals with product content embeddings\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Uses vector retrieval to enhance embeddings from multiple sources (user behavior + content metadata + support documents)\n",
      "* Supports multi-domain recommendations for enterprise applications\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Benefits of Recommendation Systems\n",
      "\n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains\n",
      "\n",
      "**RAG-Specific Benefits:**\n",
      "\n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example Applications\n",
      "\n",
      "* **E-commerce:** Personalized product recommendations (Amazon, Alibaba)\n",
      "* **Media & Streaming:** Content suggestions (Netflix, Spotify, YouTube)\n",
      "* **SaaS Platforms:** Feature or content recommendations within enterprise tools\n",
      "* **Knowledge Management:** Suggesting relevant documents, SOPs, and policies for employees\n",
      "* **Sales Enablement:** Lead prioritization and cross-selling recommendations\n",
      "* **Conversational Agents:** RAG-powered chatbots providing context-aware suggestions in multi-turn dialogues\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Average increase in click-through rate: 15‚Äì25%\n",
      "* Average revenue lift: 10‚Äì20%\n",
      "* Reduction in search effort: 30‚Äì50%\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Technologies & Frameworks Used\n",
      "\n",
      "**Machine Learning & Deep Learning:**\n",
      "\n",
      "* Scikit-learn, XGBoost, LightGBM for classical models\n",
      "* TensorFlow, PyTorch for deep learning and embedding models\n",
      "* Transformer-based embeddings (BERT, Sentence-BERT, OpenAI embeddings)\n",
      "\n",
      "**RAG & Retrieval Systems:**\n",
      "\n",
      "* Vector databases: Pinecone, Weaviate, Qdrant\n",
      "* Retrieval pipelines: Elasticsearch, FAISS, Milvus\n",
      "* Knowledge enrichment: APIs and document ingestion pipelines\n",
      "\n",
      "**Backend & Integration:**\n",
      "\n",
      "* Microservices architecture with REST/GraphQL APIs\n",
      "* Scalable pipelines for real-time recommendation inference\n",
      "* Authentication, authorization, and secure handling of sensitive user data\n",
      "\n",
      "**Monitoring & Optimization:**\n",
      "\n",
      "* Metrics dashboards for recommendation accuracy (precision, recall, NDCG, RMSE)\n",
      "* A/B testing frameworks for model evaluation\n",
      "* Continuous retraining pipelines for data drift and user behavior changes\n",
      "\n",
      "**Outcome:**\n",
      "\n",
      "* Enterprise-grade recommendation systems with high relevance, explainability, and ROI, capable of integrating into RAG-powered AI assistants and large-scale production platforms.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "## 2. Recommendation Systems  \n",
      "* **Basic Package:**  \n",
      "* Single-domain recommendations\n",
      "* Content-based or collaborative filtering\n",
      "* Simple frontend widget integration\n",
      "* Price: $30,000 ‚Äì $60,000\n",
      "* Delivery: 4‚Äì8 weeks  \n",
      "* **Standard Package:**  \n",
      "* Hybrid recommendation engine\n",
      "* Multi-source RAG-augmented embeddings\n",
      "* Real-time personalization and analytics\n",
      "* Price: $60,000 ‚Äì $120,000\n",
      "* Delivery: 8‚Äì12 weeks  \n",
      "* **Enterprise Package:**  \n",
      "* Full RAG-augmented, multi-domain recommendations\n",
      "* Scalable architecture for high-volume users\n",
      "* Multi-channel integration and reporting dashboards\n",
      "* Price: $120,000 ‚Äì $300,000+\n",
      "* Delivery: 12‚Äì20 weeks  \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get relevant documents\n",
    "\n",
    "query = 'Who can build me a recommendation system?'\n",
    "relevant_docs = base_retriever_OpenAI.invoke(query)\n",
    "\n",
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "156ee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def fetch_context_with_tracing(retriever, query: str):\n",
    "    \"\"\"Retrieves context with production-grade error handling and config.\"\"\"\n",
    "    try:\n",
    "        # Using .invoke is the standard for LCEL (LangChain Expression Language)\n",
    "        # config allows for tagging and metadata for LangSmith/Debugging\n",
    "        config = RunnableConfig(tags=[\"production-retrieval\"], metadata={\"user_id\": \"milos_01\"})\n",
    "        \n",
    "        docs = retriever.invoke(query, config=config)\n",
    "        \n",
    "        if not docs:\n",
    "            print(f\"DEBUG: No documents retrieved for query: {query}\")\n",
    "            return []\n",
    "            \n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL: Retrieval failed. Error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e48dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = fetch_context_with_tracing(base_retriever_OpenAI, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77c158b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "# Recommendation Systems Overview ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **detailed, professional overview of recommendation systems**, emphasizing enterprise-grade architectures, RAG integration, and measurable business impact. It is designed for internal knowledge, client presentations, and technical strategy alignment.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "### 1.3 Hybrid Recommendation Systems  \n",
      "**Description:**\n",
      "Hybrid systems combine collaborative and content-based approaches to maximize accuracy and coverage.  \n",
      "**Common Architectures:**  \n",
      "* Weighted hybrid: Combine scores from multiple models\n",
      "* Switching hybrid: Choose model based on context (e.g., cold-start vs active user)\n",
      "* Cascade hybrid: Rank candidates from one model, re-rank using another  \n",
      "**Enterprise Metrics:**  \n",
      "* Netflix hybrid approach improves RMSE by 10‚Äì15% over pure collaborative filtering\n",
      "* Amazon reports increased conversion rates by combining behavioral signals with product content embeddings  \n",
      "**RAG Integration:**  \n",
      "* Uses vector retrieval to enhance embeddings from multiple sources (user behavior + content metadata + support documents)\n",
      "* Supports multi-domain recommendations for enterprise applications  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "## 3. Example Applications  \n",
      "* **E-commerce:** Personalized product recommendations (Amazon, Alibaba)\n",
      "* **Media & Streaming:** Content suggestions (Netflix, Spotify, YouTube)\n",
      "* **SaaS Platforms:** Feature or content recommendations within enterprise tools\n",
      "* **Knowledge Management:** Suggesting relevant documents, SOPs, and policies for employees\n",
      "* **Sales Enablement:** Lead prioritization and cross-selling recommendations\n",
      "* **Conversational Agents:** RAG-powered chatbots providing context-aware suggestions in multi-turn dialogues  \n",
      "**Enterprise Metrics:**  \n",
      "* Average increase in click-through rate: 15‚Äì25%\n",
      "* Average revenue lift: 10‚Äì20%\n",
      "* Reduction in search effort: 30‚Äì50%  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "# Recommendation Systems Overview ‚Äì Cassiopeia Intelligence\n",
      "\n",
      "This document provides a **detailed, professional overview of recommendation systems**, emphasizing enterprise-grade architectures, RAG integration, and measurable business impact. It is designed for internal knowledge, client presentations, and technical strategy alignment.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Types of Recommendation Systems\n",
      "\n",
      "### 1.1 Collaborative Filtering\n",
      "\n",
      "**Description:**\n",
      "Collaborative filtering leverages user-item interactions to make predictions, identifying patterns in behavior across users.\n",
      "\n",
      "**Key Approaches:**\n",
      "\n",
      "* **User-based:** Recommends items liked by similar users\n",
      "* **Item-based:** Recommends items similar to those a user has interacted with\n",
      "* **Matrix factorization (SVD, ALS):** Reduces dimensionality for large datasets\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Netflix: Collaborative filtering achieves 60‚Äì70% of recommendation accuracy\n",
      "* Amazon: Item-based filtering scales to millions of products and users with >80% relevance in click-through\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Enhances collaborative models with retrieved context from product metadata or knowledge bases\n",
      "* Reduces cold-start problem by providing auxiliary information for new items/users\n",
      "\n",
      "---\n",
      "\n",
      "### 1.2 Content-Based Filtering\n",
      "\n",
      "**Description:**\n",
      "Content-based systems recommend items similar to those a user has interacted with, based on item features and metadata.\n",
      "\n",
      "**Techniques:**\n",
      "\n",
      "* TF-IDF, BM25 for textual content similarity\n",
      "* Embedding-based similarity using BERT or sentence transformers\n",
      "* Feature weighting and attribute-based scoring\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Spotify uses content-based filtering to surface new songs aligned with listener preferences\n",
      "* LinkedIn applies content-based recommendations to match users with relevant content or job postings\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Retrieves additional context from knowledge bases (product details, reviews, technical documentation) to enrich recommendations\n",
      "* Enables explainable recommendations by grounding suggestions in documented features\n",
      "\n",
      "---\n",
      "\n",
      "### 1.3 Hybrid Recommendation Systems\n",
      "\n",
      "**Description:**\n",
      "Hybrid systems combine collaborative and content-based approaches to maximize accuracy and coverage.\n",
      "\n",
      "**Common Architectures:**\n",
      "\n",
      "* Weighted hybrid: Combine scores from multiple models\n",
      "* Switching hybrid: Choose model based on context (e.g., cold-start vs active user)\n",
      "* Cascade hybrid: Rank candidates from one model, re-rank using another\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Netflix hybrid approach improves RMSE by 10‚Äì15% over pure collaborative filtering\n",
      "* Amazon reports increased conversion rates by combining behavioral signals with product content embeddings\n",
      "\n",
      "**RAG Integration:**\n",
      "\n",
      "* Uses vector retrieval to enhance embeddings from multiple sources (user behavior + content metadata + support documents)\n",
      "* Supports multi-domain recommendations for enterprise applications\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Benefits of Recommendation Systems\n",
      "\n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains\n",
      "\n",
      "**RAG-Specific Benefits:**\n",
      "\n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example Applications\n",
      "\n",
      "* **E-commerce:** Personalized product recommendations (Amazon, Alibaba)\n",
      "* **Media & Streaming:** Content suggestions (Netflix, Spotify, YouTube)\n",
      "* **SaaS Platforms:** Feature or content recommendations within enterprise tools\n",
      "* **Knowledge Management:** Suggesting relevant documents, SOPs, and policies for employees\n",
      "* **Sales Enablement:** Lead prioritization and cross-selling recommendations\n",
      "* **Conversational Agents:** RAG-powered chatbots providing context-aware suggestions in multi-turn dialogues\n",
      "\n",
      "**Enterprise Metrics:**\n",
      "\n",
      "* Average increase in click-through rate: 15‚Äì25%\n",
      "* Average revenue lift: 10‚Äì20%\n",
      "* Reduction in search effort: 30‚Äì50%\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Technologies & Frameworks Used\n",
      "\n",
      "**Machine Learning & Deep Learning:**\n",
      "\n",
      "* Scikit-learn, XGBoost, LightGBM for classical models\n",
      "* TensorFlow, PyTorch for deep learning and embedding models\n",
      "* Transformer-based embeddings (BERT, Sentence-BERT, OpenAI embeddings)\n",
      "\n",
      "**RAG & Retrieval Systems:**\n",
      "\n",
      "* Vector databases: Pinecone, Weaviate, Qdrant\n",
      "* Retrieval pipelines: Elasticsearch, FAISS, Milvus\n",
      "* Knowledge enrichment: APIs and document ingestion pipelines\n",
      "\n",
      "**Backend & Integration:**\n",
      "\n",
      "* Microservices architecture with REST/GraphQL APIs\n",
      "* Scalable pipelines for real-time recommendation inference\n",
      "* Authentication, authorization, and secure handling of sensitive user data\n",
      "\n",
      "**Monitoring & Optimization:**\n",
      "\n",
      "* Metrics dashboards for recommendation accuracy (precision, recall, NDCG, RMSE)\n",
      "* A/B testing frameworks for model evaluation\n",
      "* Continuous retraining pipelines for data drift and user behavior changes\n",
      "\n",
      "**Outcome:**\n",
      "\n",
      "* Enterprise-grade recommendation systems with high relevance, explainability, and ROI, capable of integrating into RAG-powered AI assistants and large-scale production platforms.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "## 2. Recommendation Systems  \n",
      "* **Basic Package:**  \n",
      "* Single-domain recommendations\n",
      "* Content-based or collaborative filtering\n",
      "* Simple frontend widget integration\n",
      "* Price: $30,000 ‚Äì $60,000\n",
      "* Delivery: 4‚Äì8 weeks  \n",
      "* **Standard Package:**  \n",
      "* Hybrid recommendation engine\n",
      "* Multi-source RAG-augmented embeddings\n",
      "* Real-time personalization and analytics\n",
      "* Price: $60,000 ‚Äì $120,000\n",
      "* Delivery: 8‚Äì12 weeks  \n",
      "* **Enterprise Package:**  \n",
      "* Full RAG-augmented, multi-domain recommendations\n",
      "* Scalable architecture for high-volume users\n",
      "* Multi-channel integration and reporting dashboards\n",
      "* Price: $120,000 ‚Äì $300,000+\n",
      "* Delivery: 12‚Äì20 weeks  \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd0df4",
   "metadata": {},
   "source": [
    "### Contextual Compression\n",
    "\n",
    "When using retrieval-augmented generation (RAG), **retrieved documents often contain irrelevant information** that is unrelated to the user‚Äôs query. Passing all of this information to a language model can be **costly and reduce accuracy**.  \n",
    "\n",
    "> In high-stakes enterprise RAG systems, **more data often means more noise**.\n",
    "\n",
    "For example:  \n",
    "- Suppose a retrieved document chunk is **1,600 characters**, but only **200 characters are relevant** to the query.  \n",
    "- Sending the entire chunk to the model means paying for **1,400 characters of unnecessary content**, which may **distract the model** and reduce answer quality.\n",
    "\n",
    "**Contextual Compression** solves this problem by:  \n",
    "- Extracting only the **‚Äúmeat‚Äù**‚Äîthe portions of the document that are **directly relevant** to the user‚Äôs query.  \n",
    "- Passing a **concise, focused summary** to the model instead of the full chunk.  \n",
    "\n",
    "This approach leads to:  \n",
    "- **Lower costs** (fewer tokens processed by the LLM)  \n",
    "- **Higher accuracy** (less noise for the model to process)  \n",
    "- **More efficient retrieval-augmented pipelines** in enterprise applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78780c06",
   "metadata": {},
   "source": [
    "### Contextual Compression Retriever\n",
    "\n",
    "The **Contextual Compression Retriever** is designed to **remove irrelevant information** from retrieved documents, keeping only content that is directly related to the query context. This helps reduce costs and improves the accuracy of LLM responses.\n",
    "\n",
    "---\n",
    "\n",
    "### How the Contextual Compression Retriever Works\n",
    "\n",
    "1. **Query Retrieval:**  \n",
    "   The user query is first passed to a **base retriever** (typically a vectorstore-backed retriever) which returns an initial set of documents.\n",
    "\n",
    "2. **Document Compression:**  \n",
    "   The retrieved documents are then passed through a **Document Compressor**, which either reduces the content or removes irrelevant documents entirely.  \n",
    "\n",
    "> The document compressor can make an [LLM call](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression#adding-contextual-compression-with-an-llmchainextractor) to perform **contextual compression** on each document.  \n",
    "> ‚ö†Ô∏è This approach can be slow and costly if done on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Efficient Alternative: Document Compressor Pipeline\n",
    "\n",
    "Instead of compressing entire documents directly, we can use a **Document Compressor Pipeline** to process documents more efficiently:\n",
    "\n",
    "1. **Split Documents into Chunks:**  \n",
    "   Use `CharacterTextSplitter` to break each document into smaller chunks (e.g., chunk size = 500 characters).\n",
    "\n",
    "2. **Remove Redundant Chunks:**  \n",
    "   Apply `EmbeddingsRedundantFilter` to filter out overlapping or repeated content.\n",
    "\n",
    "3. **Select Most Relevant Chunks:**  \n",
    "   Use `EmbeddingsFilter` to pick the chunks most relevant to the query based on a **similarity threshold** and **k parameter**.  \n",
    "   - For example, set `k = 16` to select the top 16 chunks.\n",
    "\n",
    "4. **Reorder Chunks for LLM Efficiency:**  \n",
    "   Use `LongContextReorder` to arrange the chunks so that the **most relevant elements appear at the top and bottom** of the list.  \n",
    "   - This ordering improves LLM performance and helps the model focus on key information first.  \n",
    "   - More details in the [Long Context paper](https://arxiv.org/abs/2307.03172).\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Using Contextual Compression\n",
    "\n",
    "- **Reduces token usage** ‚Üí lower LLM costs.  \n",
    "- **Removes irrelevant content** ‚Üí improves answer quality.  \n",
    "- **Maintains important context** while keeping documents concise.  \n",
    "- **Supports high-performance RAG pipelines** by prioritizing relevant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ddc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
