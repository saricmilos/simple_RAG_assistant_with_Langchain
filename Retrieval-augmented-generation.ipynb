{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63cf484",
   "metadata": {},
   "source": [
    "# Build an AI Chatbot with LangChain\n",
    "\n",
    "**Author:** Milos Saric [https://saricmilos.com/]  \n",
    "**Date:** January 15, 2026  \n",
    "\n",
    "Here‚Äôs the thing: most ‚Äúhow-to‚Äù guides start with the same line: *‚ÄúGrab your OpenAI API key and add a credit card.‚Äù*  \n",
    "\n",
    "But what if you **don‚Äôt want to pay**? Maybe you‚Äôre a student. Maybe you hate hitting rate limits. Or maybe you just want a chatbot that runs locally, offline, and respects your privacy.  \n",
    "\n",
    "Good news: it‚Äôs possible. In this guide, we‚Äôll show you how to build a chatbot using **LangChain, React, and TypeScript**‚Äîwith **zero cloud dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "## How AI Usually Works ‚Äî APIs and Tokens Explained\n",
    "\n",
    "Think of an **API** as a waiter in a restaurant:\n",
    "\n",
    "1. You ask the app: *‚ÄúWhat‚Äôs the capital of France?‚Äù*  \n",
    "2. The waiter (API) delivers your request to the kitchen (the AI).  \n",
    "3. The kitchen cooks up an answer.  \n",
    "4. The waiter brings it back: *‚ÄúParis!‚Äù*\n",
    "\n",
    "With OpenAI, your computer doesn‚Äôt do the thinking. Everything happens on OpenAI‚Äôs servers. The smarter the AI, the more expensive the ‚Äúmeal.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## Why Tokens Matter ‚Äî Every Word Has a Price\n",
    "\n",
    "Think of each word as a **puzzle piece**. The AI puts the pieces together to understand and respond:\n",
    "\n",
    "- Short phrases: just a couple of pieces.  \n",
    "- Complex sentences: 10‚Äì15 pieces or more.  \n",
    "\n",
    "Every puzzle piece = **token**. APIs like OpenAI charge for tokens, not messages. That‚Äôs why long chats or memory-enabled bots get expensive fast.\n",
    "\n",
    "---\n",
    "\n",
    "### What‚Äôs a Token, Exactly?\n",
    "\n",
    "A token is basically a chunk of text:\n",
    "\n",
    "- `\"Hello\"` ‚Üí 1 token  \n",
    "- `\"Artificial Intelligence is awesome\"` ‚Üí ~5‚Äì6 tokens  \n",
    "- A full conversation ‚Üí hundreds of tokens  \n",
    "- Upload a document ‚Üí thousands of tokens  \n",
    "\n",
    "Example conversation (roughly 50‚Äì80 tokens):\n",
    "\n",
    "**User:** \"Hi, I can't log in to my account\"  \n",
    "**Bot:** \"I can help! What error message are you seeing?\"  \n",
    "**User:** \"'Invalid credentials,' but my password is correct\"  \n",
    "**Bot:** \"Got it. Let's try some troubleshooting steps...\"  \n",
    "\n",
    "Multiply that by hundreds of users, and token costs can explode‚Äîeven at just $0.002 per 1K tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## Hidden Costs of Relying on Paid APIs\n",
    "\n",
    "Even beyond token fees, there are other downsides:\n",
    "\n",
    "### Scaling Costs\n",
    "- Testing & development: ~$50/month  \n",
    "- Small business: $200‚Äì500/month  \n",
    "- Enterprise: $2,000‚Äì10,000+/month  \n",
    "\n",
    "### Infrastructure & Reliability\n",
    "- Your app breaks if the API goes down  \n",
    "- Latency slows the experience  \n",
    "- Constant internet connection required  \n",
    "\n",
    "### Privacy Risks\n",
    "- All data goes through a third-party server  \n",
    "- You can‚Äôt control retention policies  \n",
    "- Compliance issues for sensitive industries  \n",
    "\n",
    "### Service Limits\n",
    "- Rate limits throttle usage  \n",
    "- Outages leave users stranded  \n",
    "- Price changes can break your model  \n",
    "\n",
    "Normally, this is where you‚Äôd start reaching for your wallet. But in this guide, **we‚Äôll avoid all of it**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1778b15",
   "metadata": {},
   "source": [
    "# What is LangChain, Really?\n",
    "\n",
    "LangChain is an **open-source framework** built to help developers create AI applications that are **context-aware, reasoning-capable, and able to use external tools**. The twist? You **don‚Äôt have to rely on cloud APIs** to make it work.  \n",
    "\n",
    "Think of LangChain like **LEGO for AI**. You get different ‚Äúblocks‚Äù that you can snap together:\n",
    "\n",
    "- **Language Model blocks** ‚Äì the brain of your AI  \n",
    "- **Memory blocks** ‚Äì to remember conversations  \n",
    "- **Tool blocks** ‚Äì calculators, web search, file access  \n",
    "- **Logic blocks** ‚Äì decision-making and reasoning  \n",
    "- **Data blocks** ‚Äì document search, databases  \n",
    "\n",
    "Whether your ‚Äúbrain‚Äù is **GPT-4**, a **local model like LLaMA**, or something running entirely on your machine, LangChain helps you assemble all the pieces into a functioning chatbot.  \n",
    "\n",
    "---\n",
    "\n",
    "## Why LangChain is Special\n",
    "\n",
    "Using LangChain with **local or open-source models** gives you benefits you can‚Äôt get with cloud APIs:\n",
    "\n",
    "- ‚úÖ No API key required  \n",
    "- ‚úÖ No token limits ‚Äì chat freely  \n",
    "- ‚úÖ No unexpected bills ‚Äì completely free after setup  \n",
    "- ‚úÖ Full privacy ‚Äì nothing leaves your computer  \n",
    "- ‚úÖ Works offline ‚Äì no internet needed  \n",
    "- ‚úÖ Total control ‚Äì fully customizable  \n",
    "\n",
    "---\n",
    "\n",
    "## Why Use LangChain? (The Big Deal)\n",
    "\n",
    "LangChain isn‚Äôt just about saving money‚Äîit‚Äôs about **flexibility, power, and simplicity**. Whether you‚Äôre a student, beginner, or seasoned developer, here‚Äôs why it‚Äôs a game-changer:\n",
    "\n",
    "\n",
    "### 1. Simplified Development\n",
    "LangChain removes the headache of writing all the low-level code yourself. It handles **connecting models, managing prompts, and storing memory** so you can focus on what your AI should actually do. Think of it as a **pre-built toolbox for AI**, ready to go.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Modular and Flexible Architecture\n",
    "Remember those LEGO blocks? That‚Äôs the core of LangChain‚Äôs design.  \n",
    "\n",
    "- Swap out models or tools with a few lines of code  \n",
    "- Add new data sources easily  \n",
    "- Experiment and iterate faster than ever  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Context Awareness and Memory\n",
    "A good chatbot remembers the conversation. LangChain comes with **built-in memory management**, so your AI keeps track of previous turns, creating **more natural and helpful interactions**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Agentic Capabilities\n",
    "This is where LangChain shines for advanced AI:  \n",
    "\n",
    "- Build AI **agents** that can reason and make decisions  \n",
    "- Use tools automatically: search the web, run calculations, execute code  \n",
    "- Create **multi-step workflows** where the AI can actually solve problems, not just answer questions  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Community and Ecosystem\n",
    "Even though it‚Äôs relatively new, LangChain has a **growing and supportive community**. Tutorials, examples, and resources are abundant. If you get stuck, chances are someone else has faced the same problem.  \n",
    "\n",
    "---\n",
    "\n",
    "LangChain isn‚Äôt just a framework‚Äîit‚Äôs a **platform for building smarter, flexible, and private AI applications** without relying on paid cloud services.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49ad56",
   "metadata": {},
   "source": [
    "# Building a RAG Chatbot with LangChain and OLLAMA APIs\n",
    "\n",
    "Large Language Models (LLMs) are incredibly powerful‚Äîthey can write essays, answer questions, and even get creative‚Äîbut they have one key limitation: **their knowledge is static**. That means they can sometimes give outdated or incorrect answers.  \n",
    "\n",
    "To overcome this, we can use **Retrieval Augmented Generation (RAG)**. RAG systems connect an LLM to **external data sources**, allowing the AI to fetch accurate, up-to-date information before generating a response.  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "The goal of this project is to build a **RAG chatbot** in **LangChain** using **OLLAMA APIs**. The chatbot can:\n",
    "\n",
    "- Accept documents in **TXT, PDF, CSV, or DOCX** formats  \n",
    "- Retrieve relevant content from your uploaded documents  \n",
    "- Provide **accurate, context-aware answers** to your questions by sending the retrieved data to the LLM  \n",
    "\n",
    "This setup ensures your chatbot isn‚Äôt just guessing‚Äîit **leverages your documents to give correct answers**.  \n",
    "\n",
    "---\n",
    "\n",
    "## How the RAG System Works\n",
    "\n",
    "We broke the system down into its main components:\n",
    "\n",
    "1. **Document Loader** ‚Äì Reads and parses your uploaded files  \n",
    "2. **Vector Store / Embeddings** ‚Äì Converts documents into a format that‚Äôs easy for the chatbot to search  \n",
    "3. **Retriever** ‚Äì Finds the most relevant pieces of your documents based on your question  \n",
    "4. **Conversational Retrieval Chain** ‚Äì Combines the retrieved content with your question and passes it to the LLM for an accurate response  \n",
    "\n",
    "---\n",
    "\n",
    "## User Interface\n",
    "\n",
    "To make the system interactive, we built a **React-based UI**. Users can:\n",
    "\n",
    "- Upload documents  \n",
    "- Ask questions about the content  \n",
    "- Receive precise answers generated by the RAG-powered LLM  \n",
    "\n",
    "This creates a **seamless chat experience** where the AI is directly informed by your data.\n",
    "\n",
    "---\n",
    "\n",
    "By combining **LangChain**, **RAG techniques**, and **OLLAMA APIs**, this project demonstrates how to create a chatbot that is **both intelligent and grounded in reliable data**, bridging the gap between static LLM knowledge and real-world information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fe011",
   "metadata": {},
   "source": [
    "![Alt text](./images/rag-chatbot-architecture-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680a288",
   "metadata": {},
   "source": [
    "# Required Libraries Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1f2a6",
   "metadata": {},
   "source": [
    "### CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb2a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea255085",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = Path(\"./data\").resolve()  # main_folder/data\n",
    "\n",
    "TMP_DIR = BASE_DATA_DIR / \"tmp\"  # main_folder/data/tmp\n",
    "DOCS_DIR = BASE_DATA_DIR / \"docs\"  # main_folder/data/docs\n",
    "VECTOR_STORE_DIR = BASE_DATA_DIR / \"vector_stores\"  # main_folder/data/vector_stores\n",
    "\n",
    "# Make sure tmp folder exists\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58970efc",
   "metadata": {},
   "source": [
    "### MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d537a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.API_KEYS import get_environment_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7d09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.langchain import (langchain_document_loader,\n",
    "                            show_random_preview, \n",
    "                            create_text_splitter,\n",
    "                             split_documents,\n",
    "                             tiktoken_tokens,\n",
    "                             select_embeddings_model,\n",
    "                             create_vectorstore,\n",
    "                             print_documents,\n",
    "                             create_advanced_markdown_splitter\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b9ffe",
   "metadata": {},
   "source": [
    "# Required API Keys for Our Application\n",
    "\n",
    "For this project, we will need **two API keys**:\n",
    "\n",
    "- **OpenAI** API key: [Get an API key](https://platform.openai.com/account/api-keys)  \n",
    "- **Google** API key: [Get an API key](https://makersuite.google.com/app/apikey)  \n",
    "\n",
    "> ‚ö†Ô∏è **Security Notice:** We will **not include our secret keys** directly in this notebook.  \n",
    ">\n",
    "> First, make sure to set the following environment variables on your system:  \n",
    "> `OPENAI_API_KEY`, `GOOGLE_API_KEY`.  \n",
    ">\n",
    "> After that, we can safely load them in Python as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5023ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[WARNING]: 'OPENAI_API_KEY' not found in environment variables.\n",
      "[INFO]: 'OPENAI_API_KEY' has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = get_environment_variable(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2968c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milos Saric is a fictional character in the video game \"Watch Dogs\". He is a former hacker who assists the main protagonist in his mission against the corrupt system.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "try:\n",
    "    client = OpenAI() # defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
    "except:\n",
    "    client = OpenAI(api_key=openai_api_key) # if OPENAI_API_KEY is not created as environment variable\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": 'Who is Milos Saric\"?'},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0ac8a",
   "metadata": {},
   "source": [
    "# Conversational RAG Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f31117",
   "metadata": {},
   "source": [
    "![Alt text](./images/rag_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bce4cd",
   "metadata": {},
   "source": [
    "## RAG Architecture: How Everything Fits Together\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) system is built around **two core building blocks**, each with a clearly defined responsibility.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Block 1: Knowledge Ingestion & Retrieval\n",
    "\n",
    "This block is responsible for **understanding and storing your data** so it can be searched efficiently later. It includes:\n",
    "\n",
    "- **Document Loader** ‚Äì Ingests external data (PDFs, text files, CSVs, DOCX, etc.)  \n",
    "- **Text Splitter** ‚Äì Breaks large documents into smaller, manageable chunks  \n",
    "- **Embedding Model** ‚Äì Converts text chunks into numerical vectors  \n",
    "- **Vector Store (Chroma)** ‚Äì Stores embeddings for fast similarity search  \n",
    "- **Retriever** ‚Äì Finds the most relevant document chunks for a given query  \n",
    "\n",
    "Together, this block transforms raw documents into a searchable knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Block 2: Reasoning, Memory & Generation\n",
    "\n",
    "This block focuses on **thinking, context, and response generation**. It consists of:\n",
    "\n",
    "- **Large Language Model (LLM)** ‚Äì Generates human-like answers  \n",
    "- **Prompt Templates** ‚Äì Structure how questions and retrieved documents are presented to the LLM  \n",
    "- **Memory** ‚Äì Keeps track of conversation history across turns  \n",
    "\n",
    "This block collaborates with the retriever to produce **accurate, context-aware answers**.\n",
    "\n",
    "---\n",
    "\n",
    "## End-to-End RAG Workflow\n",
    "\n",
    "Below is a step-by-step walkthrough of how a user‚Äôs question flows through the RAG system:\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Step 1: Question Reformulation (Steps 1‚Äì4)\n",
    "\n",
    "When a user asks a **follow-up question**, it may rely on prior context.  \n",
    "To avoid ambiguity, the system first converts it into a **standalone question**.\n",
    "\n",
    "**Example:**\n",
    "- User: *‚ÄúWhat does DTC stand for?‚Äù*  \n",
    "- AI: *‚ÄúDTC stands for Diffuse to Choose.‚Äù*  \n",
    "- Follow-up: *‚ÄúCan you explain its use cases and implementation?‚Äù*  \n",
    "\n",
    "The LLM rewrites this as:  \n",
    "> **‚ÄúWhat are the use cases and implementation of Diffuse to Choose (DTC)?‚Äù**\n",
    "\n",
    "This ensures the retriever clearly understands the query.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Step 2: Document Retrieval (Steps 5‚Äì8)\n",
    "\n",
    "- The standalone question is converted into an embedding  \n",
    "- The retriever compares it with stored embeddings in the **Chroma vector database**  \n",
    "- The most relevant document chunks are retrieved based on similarity  \n",
    "\n",
    "This step grounds the AI‚Äôs response in **actual source material**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Step 3: Prompt Augmentation & Answer Generation (Steps 9‚Äì10)\n",
    "\n",
    "- Retrieved documents are injected into the LLM prompt  \n",
    "- Chat history may also be included for continuity  \n",
    "- The augmented prompt is sent to the LLM  \n",
    "\n",
    "The LLM now generates an answer that is **both context-aware and evidence-based**.\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Step 4: Memory Update (Step 11)\n",
    "\n",
    "- The user‚Äôs follow-up question and the AI‚Äôs response are saved to memory  \n",
    "- This allows future questions to build naturally on previous interactions  \n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs Next?\n",
    "\n",
    "In the following sections, we‚Äôll take a deeper look at **each individual component**‚Äîfrom document loading and embeddings to retrievers, memory, and conversational chains‚Äîso you can fully understand how to build and customize your own RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00831008",
   "metadata": {},
   "source": [
    "# Conversational RAG Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935bb90",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Retrieval includes: document loaders, text splitting into chunks, vector stores and embeddings, and finally retrievers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce461c",
   "metadata": {},
   "source": [
    "# How to Add More Personal Information to Train Your RAG System\n",
    "\n",
    "## First: A Key Mindset Shift\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) does **not** train or fine-tune the language model itself.  \n",
    "Instead, it improves responses by **retrieving relevant documents** and injecting them into the model‚Äôs prompt.\n",
    "\n",
    "> In simple terms:  \n",
    "> **You don‚Äôt train the model ‚Äî you train the knowledge base.**\n",
    "\n",
    "The quality of your RAG chatbot depends directly on the **quality, structure, and relevance** of the documents you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## What Kind of Information Should You Add About Yourself?\n",
    "\n",
    "To get the best results, organize your personal data into **clear categories** rather than one large, unstructured file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27b257",
   "metadata": {},
   "source": [
    "## Document Loaders in LangChain\n",
    "\n",
    "Document loaders are a core component of LangChain. Their job is simple but critical: **they ingest data from different sources and convert it into a standardized document format** that the rest of the pipeline can understand.\n",
    "\n",
    "LangChain provides **80+ built-in document loaders**, making it easy to work with data from almost anywhere, including:\n",
    "\n",
    "- üåê Web pages and APIs  \n",
    "- ‚òÅÔ∏è Cloud storage services (e.g., AWS S3)  \n",
    "- üìÅ Local files such as TXT, PDF, CSV, and JSON  \n",
    "- üßæ Git repositories  \n",
    "- üìß Emails and messaging platforms  \n",
    "- üóÑÔ∏è Databases and other structured sources  \n",
    "\n",
    "You can explore the full list here:  \n",
    "üëâ [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Document Loaders Matter\n",
    "\n",
    "Raw data comes in many formats. Document loaders handle:\n",
    "- File reading and parsing  \n",
    "- Format-specific preprocessing  \n",
    "- Converting data into LangChain‚Äôs `Document` objects  \n",
    "\n",
    "This ensures downstream components‚Äîlike text splitters, embeddings, and retrievers‚Äîcan work seamlessly regardless of the data source.\n",
    "\n",
    "---\n",
    "\n",
    "## Document Loaders in Our Application\n",
    "\n",
    "For our application, we use the **`DirectoryLoader`** to load files from a temporary directory (`TMP_DIR`).\n",
    "\n",
    "This approach allows us to:\n",
    "- Upload multiple files at once  \n",
    "- Support different document formats  \n",
    "- Process all files with a single loader  \n",
    "\n",
    "Supported formats include:\n",
    "- `.txt`\n",
    "- `.pdf`\n",
    "- `.csv`\n",
    "- `.docx`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd25e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Load Summary ---\n",
      "Total Documents: 23\n",
      "  ‚Ä¢ MD: 23 files\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Load documents\n",
    "documents = langchain_document_loader(DOCS_DIR)\n",
    "\n",
    "# Professional summary reporting\n",
    "if documents:\n",
    "    counts = Counter(doc.metadata.get('source', '').split('.')[-1] for doc in documents)\n",
    "    \n",
    "    print(f\"--- Load Summary ---\")\n",
    "    print(f\"Total Documents: {len(documents)}\")\n",
    "    for ext, count in counts.items():\n",
    "        print(f\"  ‚Ä¢ {ext.upper()}: {count} files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents found. Check your directory path and file extensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24194666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf05d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Document[16]** \n",
       "\n",
       " **Page content** (first 1000 character):\n",
       "\n",
       "# Recommendation Systems Overview ‚Äì Cassiopeia Intelligence\n",
       "\n",
       "This document provides a **detailed, professional overview of recommendation systems**, emphasizing enterprise-grade architectures, RAG integration, and measurable business impact. It is designed for internal knowledge, client presentations, and technical strategy alignment.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Types of Recommendation Systems\n",
       "\n",
       "### 1.1 Collaborative Filtering\n",
       "\n",
       "**Description:**\n",
       "Collaborative filtering leverages user-item interactions to make predictions, identifying patterns in behavior across users.\n",
       "\n",
       "**Key Approaches:**\n",
       "\n",
       "* **User-based:** Recommends items liked by similar users\n",
       "* **Item-based:** Recommends items similar to those a user has interacted with\n",
       "* **Matrix factorization (SVD, ALS):** Reduces dimensionality for large datasets\n",
       "\n",
       "**Enterprise Metrics:**\n",
       "\n",
       "* Netflix: Collaborative filtering achieves 60‚Äì70% of recommendation accuracy\n",
       "* Amazon: Item-based filtering scales to millions of products and users with >80% relevance in clic ...\n",
       "\n",
       "**Metadata:**\n",
       "\n",
       "{'source': 'C:\\\\Users\\\\Milos\\\\Desktop\\\\GitHub_Kaggle_Projects\\\\simple_RAG_assistant_with_Langchain\\\\data\\\\docs\\\\recommendation-systems.md'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import Markdown\n",
    "random_document_id = random.choice(range(len(documents)))\n",
    "\n",
    "Markdown(f\"**Document[{random_document_id}]** \\n\\n **Page content** (first 1000 character):\\n\\n\" +\\\n",
    "         documents[random_document_id].page_content[0:1000] + \" ...\"  +\\\n",
    "         \"\\n\\n**Metadata:**\\n\\n\" + str(documents[random_document_id].metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70b3e9",
   "metadata": {},
   "source": [
    "# TEXT SPLITTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e27ef",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"text_splitters\">Text Splitters</a>\n",
    "\n",
    "Text splitters are tools that divide large documents into smaller sections, or **chunks**, that fit within a model's context window. Since language models can only process a limited number of tokens at a time, splitting text effectively is crucial for maintaining context and ensuring high-quality responses.\n",
    "\n",
    "In **LangChain**, text can be split in several ways:\n",
    "\n",
    "- **By tokens** ‚Äì divides text based on the number of tokens used by the model.  \n",
    "- **By characters** ‚Äì splits text according to character counts.  \n",
    "- **By code structure** ‚Äì specialized splitters exist for programming languages like Java, JavaScript, and PHP, allowing chunks to respect logical code blocks.  \n",
    "\n",
    "### Recommended Splitter for General Text\n",
    "\n",
    "For most text documents, it is recommended to use the **[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)**.  \n",
    "\n",
    "- **How it works:**  \n",
    "  The splitter uses a **list of characters or strings** (for example, `\"\\n\\n\"`, `\"\\n\"`, `\" \"`) in a specific order to recursively divide the text. It continues splitting until each chunk is small enough to fit within the model‚Äôs context window.  \n",
    "\n",
    "- **Why it‚Äôs effective:**  \n",
    "  This method preserves **semantic relationships** between paragraphs, sentences, and words by keeping related content together as much as possible. Instead of splitting arbitrarily, it prioritizes meaningful boundaries.\n",
    "\n",
    "### Chunk Overlap\n",
    "\n",
    "To ensure consistency and improve context retention, a **small overlap** between consecutive chunks is recommended. This means that some content at the end of one chunk is repeated at the start of the next.  \n",
    "\n",
    "- **Benefits:**  \n",
    "  - Helps the model maintain context across chunks.  \n",
    "  - Reduces the chance of losing important information between splits.  \n",
    "\n",
    "By using a proper text splitter and overlap, you can maximize the usefulness of documents in **retrieval-augmented generation (RAG)** workflows and other applications requiring structured text input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef633710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Splitters\n",
    "md_splitter, rec_splitter = create_advanced_markdown_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ffd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Two-Stage Splitting Process\n",
    "final_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d876e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created 205 semantic chunks ready for the Vector Store.\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    # STAGE A: Structural Split (Header-aware)\n",
    "    # Note: .split_text() returns a list of Document objects with header metadata\n",
    "    header_docs = md_splitter.split_text(doc.page_content)\n",
    "    \n",
    "    # Optional: Attach original file metadata (like 'source') to each header split\n",
    "    for h_doc in header_docs:\n",
    "        h_doc.metadata.update(doc.metadata)\n",
    "        \n",
    "    # STAGE B: Size-based Split (Recursively to fit 1600 characters)\n",
    "    # .split_documents() accepts the list of Docs from Stage A\n",
    "    sub_chunks = rec_splitter.split_documents(header_docs)\n",
    "    \n",
    "    final_chunks.extend(sub_chunks)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(final_chunks)} semantic chunks ready for the Vector Store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242151eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1070, 932, 1291, 1174, 1432, 487, 671, 759, 771, 635, 709, 714, 635, 610, 811, 1043, 1068, 534, 1054, 946, 866, 727, 727]\n"
     ]
    }
   ],
   "source": [
    "token_counts = tiktoken_tokens(documents)\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd66925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens - Average : 95\n",
      "Number of tokens - 25% percentile : 52\n",
      "Number of tokens - 50% percentile : 86\n",
      "Number of tokens - 75% percentile : 127\n",
      "\n",
      "Max_tokens for gpt-3.5-turbo: 4096\n"
     ]
    }
   ],
   "source": [
    "chunks_length = tiktoken_tokens(final_chunks,model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"Number of tokens - Average : {int(np.mean(chunks_length))}\")\n",
    "print(f\"Number of tokens - 25% percentile : {int(np.quantile(chunks_length,0.25))}\")\n",
    "print(f\"Number of tokens - 50% percentile : {int(np.quantile(chunks_length,0.5))}\")\n",
    "print(f\"Number of tokens - 75% percentile : {int(np.quantile(chunks_length,0.75))}\")\n",
    "print(\"\\nMax_tokens for gpt-3.5-turbo: 4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28336298",
   "metadata": {},
   "source": [
    "# Vectorsores and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da18bd4",
   "metadata": {},
   "source": [
    "### Text Embeddings\n",
    "\n",
    "**Text embeddings** are numerical representations of text in a high-dimensional vector space. In simpler terms, they convert words, sentences, or entire documents into a list of numbers (vectors) that capture their **semantic meaning**.  \n",
    "\n",
    "For example, OpenAI‚Äôs `text-embedding-ada-002` model produces embeddings of size **1536**, meaning each text input is represented as a vector with 1,536 numbers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why embeddings matter\n",
    "\n",
    "Embeddings allow us to **compare the meaning of different texts**. Once text is converted into vectors, we can measure similarity between them using mathematical techniques.  \n",
    "\n",
    "- **Cosine similarity** is the most commonly used metric.  \n",
    "  - Values range from `-1` (completely opposite) to `1` (exactly similar).  \n",
    "  - Higher cosine similarity means the texts are more semantically alike.  \n",
    "\n",
    "This is especially useful for:\n",
    "- **Semantic search** ‚Äì finding documents most relevant to a query.  \n",
    "- **Recommendation systems** ‚Äì suggesting similar content.  \n",
    "- **Clustering and classification** ‚Äì grouping similar texts together.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Embedding providers\n",
    "\n",
    "Several platforms provide pre-trained embedding models. LangChain supports easy integration with these services:\n",
    "\n",
    "| Provider       | Model | Vector dimension | Notes / Cost |\n",
    "|----------------|-------|----------------|--------------|\n",
    "| OpenAI         | [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/embedding-models) | 1536 | **$0.00010 per 1K tokens** |\n",
    "| Google         | [models/embedding-001](https://ai.google.dev/models/gemini?hl=en) | 768 | **Rate limit:** 1500 requests per minute |\n",
    "| Hugging Face   | [thenlper/gte-large](https://huggingface.co/thenlper/gte-large) | 1024 | **Free** |\n",
    "\n",
    "> üí° **Tip:** Larger vector dimensions often capture more nuanced meaning but may be more computationally expensive for storage and similarity calculations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a92c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_openai = select_embeddings_model(LLM_service=\"OpenAI\", openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349f1894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between sentence 0 and 1: 0.899\n",
      "Similarity between sentence 0 and 2: 0.711\n",
      "Similarity between sentence 1 and 2: 0.712\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I want to become the world greatest data scientist.\",\n",
    "    \"I love data science.\",\n",
    "    \"How many hours should you walk per day?\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Generate embeddings\n",
    "# -------------------------------\n",
    "# embeddings_google is assumed to be initialized already\n",
    "embedding_vectors = [embeddings_openai.embed_query(sentence) for sentence in sentences]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Calculate pairwise similarity\n",
    "# -------------------------------\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Iterate over all sentence pairs\n",
    "for i, j in combinations(range(len(sentences)), 2):\n",
    "    sim_score = round(cosine_similarity(embedding_vectors[i], embedding_vectors[j]), 3)\n",
    "    print(f\"Similarity between sentence {i} and {j}: {sim_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92914301",
   "metadata": {},
   "source": [
    "### Vectorstores\n",
    "\n",
    "A **vectorstore** is a specialized type of database designed to store **embedding vectors** ‚Äî the numerical representations of text, images, or other data.  \n",
    "\n",
    "Unlike traditional databases that search by exact matches or keywords, vectorstores allow you to **search for the items that are most semantically similar** to a query. This is done by comparing the embeddings of the query with the stored embeddings, typically using metrics like **cosine similarity** or **Euclidean distance**.\n",
    "\n",
    "There are several open-source and commercial vectorstore options available. For this guide, we will use **[Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma)**, a lightweight and efficient vector database that integrates seamlessly with LangChain.\n",
    "\n",
    "> üí° **Tip:** Vectorstores are a key component of modern AI workflows, such as **semantic search**, **question answering**, and **retrieval-augmented generation (RAG)**, because they allow models to find relevant information quickly and accurately based on meaning, not just keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b843a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted at: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\vector_stores\\milossaric_vectorstore\n"
     ]
    }
   ],
   "source": [
    "vectorstore_name = \"milossaric_vectorstore\"\n",
    "vector_store = create_vectorstore(\n",
    "    embeddings=embeddings_openai,\n",
    "    documents=final_chunks,\n",
    "    vectorstore_name=vectorstore_name,\n",
    "    vectorstore_dir=VECTOR_STORE_DIR\n",
    ")\n",
    "\n",
    "print(f\"Vector store created and persisted at: {VECTOR_STORE_DIR / vectorstore_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "938928c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_OpenAI: 441 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milos\\AppData\\Local\\Temp\\ipykernel_2732\\2820668442.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vector_store_OpenAI = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Define the full path to the persisted vector store\n",
    "vector_store_path = VECTOR_STORE_DIR / \"milossaric_vectorstore\"\n",
    "\n",
    "# Load the persisted vector store\n",
    "vector_store_OpenAI = Chroma(\n",
    "    persist_directory=vector_store_path.as_posix(),\n",
    "    embedding_function=embeddings_openai\n",
    ")\n",
    "\n",
    "# Print the number of vectors/chunks in the store\n",
    "print(\"vector_store_OpenAI:\", vector_store_OpenAI._collection.count(), \"chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7ba7c",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "`Similarity search` is a technique used to find documents that are most relevant or similar to a given query. Unlike simple keyword matching, it relies on **embedding vectors**, which are numerical representations of the semantic meaning of text.  \n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Query Embedding:**  \n",
    "   The input question or query is converted into an embedding vector using the same embedding model as the vector store. This vector captures the meaning of the query in a numerical form.\n",
    "\n",
    "2. **Comparison with Vector Store:**  \n",
    "   The query embedding is compared against all document embeddings stored in the vector store. Similarity is usually measured using metrics like **cosine similarity**, which evaluates how close vectors are in the embedding space.\n",
    "\n",
    "3. **Selecting Top Matches:**  \n",
    "   The system selects the **k most similar documents** to the query (by default, k = 4). These documents are considered most likely to contain information relevant to the query.\n",
    "\n",
    "4. **Providing Context to LLMs:**  \n",
    "   The retrieved documents are sent along with the query to a **large language model (LLM)** ‚Äî such as ChatGPT, Google Gemini, or others. Providing these relevant documents helps the LLM generate **more accurate and informed responses**.\n",
    "\n",
    "5. **Optimizing Retrieval:**  \n",
    "   To improve efficiency and relevance, we can later apply **contextual compression**, which condenses the retrieved documents to only the most essential information while preserving context for the LLM.\n",
    "\n",
    "**Summary:**  \n",
    "Similarity search acts as a **bridge between your knowledge base and the LLM**, ensuring that the model has access to the most relevant information when answering queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9838a2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "Score: 0.293\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "Score: 0.293\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "Score: 0.354\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "Score: 0.354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Who can make me a recommendation system?\"\n",
    "\n",
    "# Retrieve the top 4 most similar documents with scores\n",
    "# Using cosine similarity (lower distance = more similar)\n",
    "docs_with_scores = vector_store_OpenAI.similarity_search_with_score(query, k=4)\n",
    "\n",
    "# Print the results with scores\n",
    "print_documents(docs_with_scores, search_with_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2cc84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_and_log_relevance(query, docs_with_scores, embeddings_model):\n",
    "    \"\"\"\n",
    "    Computes semantic similarity using a vectorized dot product \n",
    "    and logs a formatted relevance report.\n",
    "    \"\"\"\n",
    "    # 1. Generate Embeddings (Vectorized)\n",
    "    query_vector = embeddings_model.embed_query(query)\n",
    "    document_texts = [doc[0].page_content for doc in docs_with_scores]\n",
    "    doc_matrix = np.array(embeddings_model.embed_documents(document_texts))\n",
    "\n",
    "    # 2. Vectorized Similarity Calculation (Dot Product)\n",
    "    # Using np.inner for high-performance vector-matrix multiplication\n",
    "    relevance_scores = np.inner(query_vector, doc_matrix)\n",
    "\n",
    "    # 3. Professional Report Formatting\n",
    "    print(f\"{'='*20} RETRIEVAL RELEVANCE REPORT {'='*20}\")\n",
    "    print(f\"QUERY: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    for idx, score in enumerate(relevance_scores):\n",
    "        source = docs_with_scores[idx][0].metadata.get('source', 'Unknown')\n",
    "        # Display as normalized relevance (assuming unit vectors)\n",
    "        print(f\"RANK {idx+1} | SCORE: {score:.4f} | SOURCE: {source}\")\n",
    "        \n",
    "    print(f\"{'='*68}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9756d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== RETRIEVAL RELEVANCE REPORT ====================\n",
      "QUERY: \"Who can make me a recommendation system?\"\n",
      "\n",
      "RANK 1 | SCORE: 0.8533 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "RANK 2 | SCORE: 0.8533 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "RANK 3 | SCORE: 0.8229 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "RANK 4 | SCORE: 0.8231 | SOURCE: C:\\Users\\Milos\\Desktop\\GitHub_Kaggle_Projects\\simple_RAG_assistant_with_Langchain\\data\\docs\\services-solutions.md\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execute the reporting function\n",
    "compute_and_log_relevance(query, docs_with_scores, embeddings_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb72d28",
   "metadata": {},
   "source": [
    "# Maximum Marginal Relevance (MMR) Search\n",
    "\n",
    "Maximum Marginal Relevance (MMR) is a technique used in search and information retrieval to **select documents that are both relevant to a query and diverse among themselves**. This helps avoid redundancy in search results.\n",
    "\n",
    "- **Relevance:** How closely a document matches the query.\n",
    "- **Diversity:** How different each selected document is from the others.\n",
    "\n",
    "MMR balances these two aspects. It ensures that the selected documents are not only similar to your query but also cover different perspectives or topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3fabdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Model Selection  \n",
      "### 2.1 Classical Models  \n",
      "* **Collaborative Filtering:** User-based, item-based, matrix factorization (SVD, ALS)\n",
      "* **Content-Based Models:** TF-IDF, BM25, feature-based similarity\n",
      "* **Hybrid Approaches:** Weighted, cascade, or switching models\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "## 3. Embedding Strategy  \n",
      "* **Embedding Selection:** Choose embedding models aligned with domain-specific needs (e.g., OpenAI embeddings, Sentence-BERT, GPT-based embeddings).\n",
      "* **Vector Dimensionality:** Balance performance and retrieval accuracy by selecting appropriate embedding dimensions (typically 768‚Äì1536 for most applications).\n",
      "* **Semantic Consistency:** Ensure similar content produces close embeddings for accurate retrieval.\n",
      "* **Batch Processing:** Embed documents in batches to optimize processing time and maintain uniformity.\n",
      "* **Periodic Updates:** Regularly re-embed updated content to maintain knowledge freshness in the RAG system.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "### 2.1 Roles & Expertise\n",
      "- **Lead Engineer:**\n",
      "- Specializes in vector databases (Pinecone, Weaviate)\n",
      "- Designs hybrid recommendation pipelines and embedding strategies\n",
      "- **Full-Stack Team:**\n",
      "- Backend: FastAPI / Django / Node.js microservices\n",
      "- Frontend: React / Next.js dashboards with real-time analytics\n",
      "- Continuous integration and deployment pipelines (CI/CD)\n",
      "- **Data Scientists:**\n",
      "- Model selection, hyperparameter tuning, and evaluation\n",
      "- Embedding selection (OpenAI, Hugging Face gte-large)\n",
      "- Evaluation metrics: Precision@K, Recall@K, NDCG@K\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "## 5. Deployment Options  \n",
      "### 5.1 Cloud Deployment  \n",
      "* AWS, GCP, Azure for scalable, managed infrastructure\n",
      "* Serverless functions for event-driven RAG pipelines\n",
      "* Kubernetes (EKS, GKE, AKS) for containerized microservices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'what is Diffuse to Choose?'\n",
    "docs_MMR = vector_store_OpenAI.max_marginal_relevance_search(query,k=4)\n",
    "\n",
    "print_documents(docs_MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4d1cd",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"retrievers\">Retrievers</a>\n",
    "\n",
    "A **retriever** is a component in a search or question-answering system that is responsible for **finding and returning documents relevant to a user‚Äôs query**.  \n",
    "\n",
    "Think of a retriever as the first step in a system that answers questions: it **finds the right documents**, which can then be processed further, e.g., summarized or used by a language model.\n",
    "\n",
    "For more details, you can check the [LangChain retrievers documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/).\n",
    "\n",
    "---\n",
    "\n",
    "### How Retrievers Work\n",
    "\n",
    "1. **Receive a query**: The retriever takes a user query as input.\n",
    "2. **Search documents**: It searches a collection of documents (could be a database, a vector store, or a traditional search engine).\n",
    "3. **Return relevant documents**: It outputs documents that are most likely to answer the query.\n",
    "\n",
    "> Retrievers **do not generate new text**; they only fetch existing information.  \n",
    "\n",
    "---\n",
    "\n",
    "### Types of Retrievers\n",
    "\n",
    "1. **Vectorstore-backed Retriever** (Semantic Search)  \n",
    "   - Uses **embeddings** to represent documents and queries as high-dimensional vectors.\n",
    "   - Measures similarity between query vectors and document vectors.\n",
    "   - Returns documents that are **semantically similar**, even if exact words do not match.\n",
    "\n",
    "2. **Keyword-based Retriever**  \n",
    "   - Uses traditional keyword matching (like search engines or databases).\n",
    "   - Simple but may miss documents that **use different wording** for the same concept.\n",
    "\n",
    "3. **Hybrid Retriever**  \n",
    "   - Combines vector search and keyword search for **more accurate retrieval**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c059dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal, Dict, Any\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "def get_vectorstore_retriever(\n",
    "    vectorstore: VectorStore,\n",
    "    search_type: Literal[\"similarity\", \"mmr\", \"similarity_score_threshold\"] = \"similarity\",\n",
    "    k: int = 4,\n",
    "    score_threshold: Optional[float] = None,\n",
    "    fetch_k: int = 20\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"Standardizes the instantiation of vectorstore-backed retrievers.\n",
    "\n",
    "    Args:\n",
    "        vectorstore: The initialized LangChain VectorStore instance.\n",
    "        search_type: Algorithm for retrieval. \n",
    "            - 'similarity': Standard cosine/L2 distance.\n",
    "            - 'mmr': Max Marginal Relevance (diversifies results).\n",
    "            - 'similarity_score_threshold': Filters results by absolute score.\n",
    "        k: The number of final documents to return to the LLM.\n",
    "        score_threshold: Minimum relevance score required (range 0.0 to 1.0).\n",
    "            Only utilized when search_type is 'similarity_score_threshold'.\n",
    "        fetch_k: Amount of documents to pass to the MMR algorithm for reranking.\n",
    "\n",
    "    Returns:\n",
    "        BaseRetriever: A configured retriever object ready for RAG chains.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define search parameters with sensible defaults for enterprise RAG\n",
    "    search_kwargs: Dict[str, Any] = {\"k\": k}\n",
    "    \n",
    "    if search_type == \"mmr\":\n",
    "        search_kwargs[\"fetch_k\"] = fetch_k\n",
    "        \n",
    "    if score_threshold is not None:\n",
    "        search_kwargs[\"score_threshold\"] = score_threshold\n",
    "\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=search_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc220b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search\n",
    "base_retriever_OpenAI = get_vectorstore_retriever(vector_store_OpenAI,\"similarity\",k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d54694d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get relevant documents\n",
    "\n",
    "query = 'Who can build me a recommendation system?'\n",
    "relevant_docs = base_retriever_OpenAI.invoke(query)\n",
    "\n",
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "156ee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def fetch_context_with_tracing(retriever, query: str):\n",
    "    \"\"\"Retrieves context with production-grade error handling and config.\"\"\"\n",
    "    try:\n",
    "        # Using .invoke is the standard for LCEL (LangChain Expression Language)\n",
    "        # config allows for tagging and metadata for LangSmith/Debugging\n",
    "        config = RunnableConfig(tags=[\"production-retrieval\"], metadata={\"user_id\": \"milos_01\"})\n",
    "        \n",
    "        docs = retriever.invoke(query, config=config)\n",
    "        \n",
    "        if not docs:\n",
    "            print(f\"DEBUG: No documents retrieved for query: {query}\")\n",
    "            return []\n",
    "            \n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL: Retrieval failed. Error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e48dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = fetch_context_with_tracing(base_retriever_OpenAI, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77c158b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "## 2. Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd0df4",
   "metadata": {},
   "source": [
    "### Contextual Compression\n",
    "\n",
    "When using retrieval-augmented generation (RAG), **retrieved documents often contain irrelevant information** that is unrelated to the user‚Äôs query. Passing all of this information to a language model can be **costly and reduce accuracy**.  \n",
    "\n",
    "> In high-stakes enterprise RAG systems, **more data often means more noise**.\n",
    "\n",
    "For example:  \n",
    "- Suppose a retrieved document chunk is **1,600 characters**, but only **200 characters are relevant** to the query.  \n",
    "- Sending the entire chunk to the model means paying for **1,400 characters of unnecessary content**, which may **distract the model** and reduce answer quality.\n",
    "\n",
    "**Contextual Compression** solves this problem by:  \n",
    "- Extracting only the **‚Äúmeat‚Äù**‚Äîthe portions of the document that are **directly relevant** to the user‚Äôs query.  \n",
    "- Passing a **concise, focused summary** to the model instead of the full chunk.  \n",
    "\n",
    "This approach leads to:  \n",
    "- **Lower costs** (fewer tokens processed by the LLM)  \n",
    "- **Higher accuracy** (less noise for the model to process)  \n",
    "- **More efficient retrieval-augmented pipelines** in enterprise applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78780c06",
   "metadata": {},
   "source": [
    "### Contextual Compression Retriever\n",
    "\n",
    "The **Contextual Compression Retriever** is designed to **remove irrelevant information** from retrieved documents, keeping only content that is directly related to the query context. This helps reduce costs and improves the accuracy of LLM responses.\n",
    "\n",
    "---\n",
    "\n",
    "### How the Contextual Compression Retriever Works\n",
    "\n",
    "1. **Query Retrieval:**  \n",
    "   The user query is first passed to a **base retriever** (typically a vectorstore-backed retriever) which returns an initial set of documents.\n",
    "\n",
    "2. **Document Compression:**  \n",
    "   The retrieved documents are then passed through a **Document Compressor**, which either reduces the content or removes irrelevant documents entirely.  \n",
    "\n",
    "> The document compressor can make an [LLM call](https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression#adding-contextual-compression-with-an-llmchainextractor) to perform **contextual compression** on each document.  \n",
    "> ‚ö†Ô∏è This approach can be slow and costly if done on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Efficient Alternative: Document Compressor Pipeline\n",
    "\n",
    "Instead of compressing entire documents directly, we can use a **Document Compressor Pipeline** to process documents more efficiently:\n",
    "\n",
    "1. **Split Documents into Chunks:**  \n",
    "   Use `CharacterTextSplitter` to break each document into smaller chunks (e.g., chunk size = 500 characters).\n",
    "\n",
    "2. **Remove Redundant Chunks:**  \n",
    "   Apply `EmbeddingsRedundantFilter` to filter out overlapping or repeated content.\n",
    "\n",
    "3. **Select Most Relevant Chunks:**  \n",
    "   Use `EmbeddingsFilter` to pick the chunks most relevant to the query based on a **similarity threshold** and **k parameter**.  \n",
    "   - For example, set `k = 16` to select the top 16 chunks.\n",
    "\n",
    "4. **Reorder Chunks for LLM Efficiency:**  \n",
    "   Use `LongContextReorder` to arrange the chunks so that the **most relevant elements appear at the top and bottom** of the list.  \n",
    "   - This ordering improves LLM performance and helps the model focus on key information first.  \n",
    "   - More details in the [Long Context paper](https://arxiv.org/abs/2307.03172).\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Using Contextual Compression\n",
    "\n",
    "- **Reduces token usage** ‚Üí lower LLM costs.  \n",
    "- **Removes irrelevant content** ‚Üí improves answer quality.  \n",
    "- **Maintains important context** while keeping documents concise.  \n",
    "- **Supports high-performance RAG pipelines** by prioritizing relevant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71abb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# 1. Base interfaces (Standardized in langchain-core)\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# 2. Retrieval Orchestration (Now explicitly in the main langchain package)\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import (\n",
    "    DocumentCompressorPipeline,\n",
    "    EmbeddingsFilter,\n",
    ")\n",
    "\n",
    "# 3. Specialized Package Imports\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    LongContextReorder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f12c4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compression_retriever(\n",
    "    embeddings: Embeddings,\n",
    "    base_retriever: BaseRetriever,\n",
    "    chunk_size: int = 500,\n",
    "    k: int = 16,\n",
    "    similarity_threshold: Optional[float] = None,\n",
    ") -> ContextualCompressionRetriever:\n",
    "    \"\"\"\n",
    "    Factory function to construct a high-precision retrieval pipeline using \n",
    "    Contextual Compression.\n",
    "\n",
    "    The pipeline follows a four-stage transformation process:\n",
    "    1. Splitting: Breaks docs into granular chunks for precise matching.\n",
    "    2. De-duplication: Removes semantically redundant information.\n",
    "    3. Relevance Filtering: Enforces a similarity threshold and limits top-k.\n",
    "    4. Reordering: Optimizes for the \"Lost in the Middle\" phenomenon.\n",
    "\n",
    "    Args:\n",
    "        embeddings: The embedding model used for redundancy and relevance filtering.\n",
    "        base_retriever: The primary vector store retriever.\n",
    "        chunk_size: Target size for internal document splitting.\n",
    "        k: Maximum number of documents to return after filtering.\n",
    "        similarity_threshold: Minimum cosine similarity score (0.0 to 1.0).\n",
    "\n",
    "    Returns:\n",
    "        A configured ContextualCompressionRetriever instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize transformers with explicit configurations\n",
    "    transformers = [\n",
    "        CharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_size // 10,  # Added overlap for context retention\n",
    "            separator=\". \"\n",
    "        ),\n",
    "        EmbeddingsRedundantFilter(embeddings=embeddings),\n",
    "        EmbeddingsFilter(\n",
    "            embeddings=embeddings, \n",
    "            k=k, \n",
    "            similarity_threshold=similarity_threshold\n",
    "        ),\n",
    "        LongContextReorder()\n",
    "    ]\n",
    "\n",
    "    # Encapsulate logic in a DocumentCompressorPipeline\n",
    "    pipeline_compressor = DocumentCompressorPipeline(transformers=transformers)\n",
    "\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7ecdc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "## 2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compression_retriever_OPENAI = create_compression_retriever(\n",
    "    embeddings=embeddings_openai,\n",
    "    base_retriever=base_retriever_OpenAI,\n",
    "    k=16)\n",
    "\n",
    "query = 'Who can build me a recommendation system?'\n",
    "\n",
    "compressed_docs = compression_retriever_OPENAI.invoke(query)\n",
    "\n",
    "print_documents(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce13b6",
   "metadata": {},
   "source": [
    "### Cohere Reranker\n",
    "\n",
    "The Cohere Reranker improves retrieval results by taking the documents returned by a base retriever and reordering them based on their semantic relevance to the user‚Äôs query. It uses Cohere‚Äôs rerank API to score and prioritize the most relevant documents, ensuring the top results best match the query‚Äôs intent. In the original LangChain setup, this was done by wrapping the retriever with a `ContextualCompressionRetriever`, which applied the reranking automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5efe2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "def create_cohere_rerank_retriever(\n",
    "    base_retriever: BaseRetriever, \n",
    "    cohere_api_key: str, \n",
    "    model: str = \"rerank-multilingual-v3.0\", \n",
    "    top_n: int = 8\n",
    ") -> ContextualCompressionRetriever:\n",
    "    \"\"\"\n",
    "    Configures a ContextualCompressionRetriever using Cohere's Rerank API.\n",
    "    \n",
    "    This factory function enhances a standard retriever by adding a secondary \n",
    "    relevance-based scoring pass. It is particularly effective at reducing noise \n",
    "    and improving the performance of the downstream LLM.\n",
    "\n",
    "    Args:\n",
    "        base_retriever: The initial retriever (e.g., VectorStoreRetriever).\n",
    "        cohere_api_key: The API key for authenticating with Cohere.\n",
    "        model: The Cohere model to use. Note: 'rerank-multilingual-v3.0' \n",
    "               is the current state-of-the-art for cross-lingual tasks.\n",
    "        top_n: The number of refined documents to return to the LLM.\n",
    "\n",
    "    Returns:\n",
    "        A ContextualCompressionRetriever instance integrated with Cohere Rerank.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the reranker compressor\n",
    "    compressor = CohereRerank(\n",
    "        cohere_api_key=cohere_api_key, \n",
    "        model=model, \n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    # Wrap the base retriever with the compressor\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82d7d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO]: 'COHERE_API_KEY' retrieved successfully from environment variables.\n"
     ]
    }
   ],
   "source": [
    "cohere_api_key = get_environment_variable(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "072668ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking using Cohere. Vcetorstore using Google Embeddings:\n",
      "\n",
      "Document 1:\n",
      "\n",
      "### 1.1 Recommendation Systems\n",
      "- **Overview:** Custom collaborative, content-based, and hybrid recommendation engines.\n",
      "- **Capabilities:**\n",
      "- Collaborative filtering (Matrix Factorization, ALS, SVD)\n",
      "- Content-based filtering (TF-IDF, embeddings with BERT / GPT models)\n",
      "- Hybrid models combining collaborative + content signals\n",
      "- RAG-enhanced recommendation for multi-source reasoning\n",
      "- **Integration:** API-first design compatible with FastAPI, Django, or Node.js backends.\n",
      "- **Use Cases:** E-commerce, Book Discovery platforms, SaaS personalization.\n",
      "- **Reference Documents:**\n",
      "- ‚ÄúRecommendation Systems Overview‚Äù\n",
      "- ‚ÄúRecommendation System Implementation Guide‚Äù\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "# Recommendation System Implementation ‚Äì Cassiopeia Intelligence  \n",
      "This document provides a **overview of recommendation system implementation**, emphasizing production-grade architectures, RAG integration, and enterprise best practices.  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "## 2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "### 2.2 Project History\n",
      "- Successfully delivered recommendation systems for:\n",
      "- **Online Retail:** Personalized product recommendations increasing CTR by 45%\n",
      "- **Book Discovery Platforms:** Multi-source content-based and collaborative filtering pipelines\n",
      "- Reference: ‚ÄúCase Study: Recommendation System‚Äù  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Benefits of Recommendation Systems  \n",
      "* **Increased Engagement:** Personalized suggestions boost user interaction (Netflix: +30‚Äì40% engagement metrics)\n",
      "* **Higher Conversion Rates:** Relevant recommendations increase sales and retention (Amazon: +20% incremental revenue)\n",
      "* **Improved User Satisfaction:** Reduces decision fatigue by surfacing relevant items quickly\n",
      "* **Enhanced Data Utilization:** Leverages existing user and content data to deliver actionable insights\n",
      "* **Scalability & Adaptability:** Supports millions of users and items across different domains  \n",
      "**RAG-Specific Benefits:**  \n",
      "* Reduces cold-start issues with knowledge-augmented retrieval\n",
      "* Improves recommendation explainability and trustworthiness\n",
      "* Supports multi-turn, conversational recommendations in chatbots and virtual assistants  \n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "## 2. Who Can Build a Recommendation System?  \n",
      "The **Cassiopeia Intelligence Engineering Team** manages all recommendation system projects from conception to production deployment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_Cohere_google = create_cohere_rerank_retriever(\n",
    "    base_retriever=base_retriever_OpenAI, \n",
    "    cohere_api_key=cohere_api_key, model=\"rerank-multilingual-v3.0\",  \n",
    "    top_n=8\n",
    ")\n",
    "\n",
    "query = 'Who can build me a recommendation system?'\n",
    "\n",
    "docs_cohere = compression_retriever_OPENAI.invoke(query)\n",
    "\n",
    "print(\"Reranking using Cohere. Vcetorstore using Google Embeddings:\\n\")\n",
    "print_documents(docs_cohere)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdec14",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"retrieval\">Retrieval: put it all together</a>\n",
    "\n",
    "Retrieval is the process of taking a user query and finding the most relevant information from a collection of documents. So far, we have looked at the individual steps:\n",
    "\n",
    "1. **Loading the data** ‚Äì bringing in the documents from a source, whether it‚Äôs a local file, database, or vectorstore.\n",
    "2. **Embedding the query** ‚Äì converting the user‚Äôs query into a vector representation that can be compared to document vectors.\n",
    "3. **Finding similar documents** ‚Äì using a base retriever to identify documents whose embeddings are closest to the query embedding.\n",
    "4. **Reordering and filtering** ‚Äì applying techniques like semantic reranking, redundancy filtering, and contextual reordering to ensure the results are the most relevant and concise.\n",
    "\n",
    "Putting it all together means combining these steps into a single retrieval pipeline. A query goes into the system, the retriever fetches candidate documents, and any compressors or rerankers refine the results. The final output is a ranked list of documents that are both relevant and contextually optimized, ready to be used for tasks like answering questions, generating summaries, or feeding into an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal, Dict, Any, List\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "# Standard LangChain Imports\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"RAG_Pipeline\")\n",
    "\n",
    "def retrieval_blocks(\n",
    "    create_new_store: bool = True,\n",
    "    llm_service: str = \"OpenAI\",\n",
    "    vectorstore_name: str = \"milossaric_vectorstore\",\n",
    "    chunk_size: int = 1600,\n",
    "    chunk_overlap: int = 200,\n",
    "    retriever_type: Literal[\"base\", \"compression\", \"cohere\"] = \"cohere\",\n",
    "    search_type: str = \"similarity\",\n",
    "    k: int = 10,\n",
    "    cohere_api_key: Optional[str] = None,\n",
    "    cohere_model: str = \"rerank-multilingual-v3.0\",\n",
    "    cohere_top_n: int = 8,\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"\n",
    "    Unified Retrieval Pipeline:\n",
    "    1. Loads documents from DOCS_DIR.\n",
    "    2. Performs Two-Stage Splitting (Markdown Header Aware -> Recursive).\n",
    "    3. Manages Chroma VectorStore (Create or Load).\n",
    "    4. Configures the final Retriever (Base, Compressed, or Cohere Rerank).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- STAGE 1: Embeddings Setup ---\n",
    "        embeddings = select_embeddings_model(\n",
    "            LLM_service=llm_service,\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # --- STAGE 2: Vectorstore Creation or Loading ---\n",
    "        if create_new_store:\n",
    "            logger.info(\"Initializing ingestion: Loading documents...\")\n",
    "            documents = langchain_document_loader(DOCS_DIR)\n",
    "            \n",
    "            # Summary Reporting\n",
    "            counts = Counter(doc.metadata.get('source', '').split('.')[-1] for doc in documents)\n",
    "            print(f\"--- Load Summary: {len(documents)} Total Docs ---\")\n",
    "            for ext, count in counts.items():\n",
    "                print(f\"  ‚Ä¢ {ext.upper()}: {count} files\")\n",
    "\n",
    "            # Two-Stage Splitting Logic\n",
    "            md_splitter, rec_splitter = create_advanced_markdown_splitter(\n",
    "                chunk_size=chunk_size, \n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            \n",
    "            final_chunks = []\n",
    "            for doc in documents:\n",
    "                # Stage A: Header-aware Structural Split\n",
    "                header_docs = md_splitter.split_text(doc.page_content)\n",
    "                for h_doc in header_docs:\n",
    "                    h_doc.metadata.update(doc.metadata) # Preserve source metadata\n",
    "                \n",
    "                # Stage B: Recursive split to size\n",
    "                sub_chunks = rec_splitter.split_documents(header_docs)\n",
    "                final_chunks.extend(sub_chunks)\n",
    "            \n",
    "            print(f\"‚úÖ Created {len(final_chunks)} semantic chunks.\")\n",
    "\n",
    "            # Create and Persist Store\n",
    "            vector_store = create_vectorstore(\n",
    "                embeddings=embeddings,\n",
    "                documents=final_chunks,\n",
    "                vectorstore_name=vectorstore_name,\n",
    "                vectorstore_dir=VECTOR_STORE_DIR\n",
    "            )\n",
    "        else:\n",
    "            # Load existing store\n",
    "            store_path = VECTOR_STORE_DIR / vectorstore_name\n",
    "            logger.info(f\"Loading existing vectorstore from {store_path}\")\n",
    "            vector_store = Chroma(\n",
    "                persist_directory=store_path.as_posix(),\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "\n",
    "        # --- STAGE 3: Retriever Configuration ---\n",
    "        # Base vectorstore retriever\n",
    "        base_retriever = get_vectorstore_retriever(\n",
    "            vectorstore=vector_store,\n",
    "            search_type=search_type,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        if retriever_type == \"base\":\n",
    "            retriever = base_retriever\n",
    "        \n",
    "        elif retriever_type == \"compression\":\n",
    "            retriever = create_compression_retriever(\n",
    "                embeddings=embeddings,\n",
    "                base_retriever=base_retriever,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "        elif retriever_type == \"cohere\":\n",
    "            if not cohere_api_key:\n",
    "                raise ValueError(\"Cohere API key is missing for reranker.\")\n",
    "            \n",
    "            retriever = create_cohere_rerank_retriever(\n",
    "                base_retriever=base_retriever,\n",
    "                cohere_api_key=cohere_api_key,\n",
    "                model=cohere_model,\n",
    "                top_n=cohere_top_n\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid retriever_type: {retriever_type}\")\n",
    "\n",
    "        print(f\"\\nüöÄ {retriever_type.upper()} pipeline created successfully!\")\n",
    "        return retriever\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build retrieval block: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc56473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
